\documentclass[12pt]{article}
\usepackage[margin=1.0in]{geometry}
\pdfpagewidth 8.5in
\pdfpageheight 11.0in
\usepackage{lingmacros}
\usepackage{tree-dvips}
\usepackage[nottoc]{tocbibind}
\usepackage{natbib}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{amstext}
\usepackage{bm}
\usepackage{mathtools}
\usepackage{float}
\usepackage{verbatim}
\begin{document}
\pagenumbering{gobble}

\newcommand\aj{Astronomical Journal}%        % Astronomical Journal 
\newcommand\araa{Annual Review of Astronomy and Astrophysics}%  % Annual Review of Astron and Astrophys 
\newcommand\apj{Astrophysical Journal}%    % Astrophysical Journal 
\newcommand\apjl{Astrophysical Journal Letters}%     % Astrophysical Journal, Letters % MK Edit 2016-01-15: the macro above didn't seem to be defined
\newcommand\apjs{Astrophysical Journal Supplement}%    % Astrophysical Journal, Supplement 
\newcommand\apss{Astrophysics and Space Science}%  % Astrophysics and Space Science 
\newcommand\aap{Astronomy and Astrophysics}%     % Astronomy and Astrophysics 
\newcommand\aapr{Astronomy and Astrophysics Reviews}%  % Astronomy and Astrophysics Reviews 
\newcommand\aaps{Astronomy and Astrophysics Supplement}%    % Astronomy and Astrophysics, Supplement 
\newcommand\icarus{Icarus}% % Icarus
\newcommand\mnras{Monthly Notices of the Royal Astronomical Society}%   % Monthly Notices of the RAS 
\newcommand\pra{Phys.~Rev.~A}% % Physical Review A: General Physics 
\newcommand\prb{Phys.~Rev.~B}% % Physical Review B: Solid State 
\newcommand\prc{Phys.~Rev.~C}% % Physical Review C 
\newcommand\prd{Phys.~Rev.~D}% % Physical Review D 
\newcommand\pre{Phys.~Rev.~E}% % Physical Review E 
\newcommand\prl{Phys.~Rev.~Lett.}% % Physical Review Letters 
\newcommand\actaa{Acta Astronomica}%  % Acta Astronomica
\newcommand\na{New~Astronomy}%  % New Astronomy

% Override choices in \autoref
\def\sectionautorefname{Section}
\def\subsectionautorefname{Section}
\def\subsubsectionautorefname{Section}

\newcommand{\msolar}{\mathrm{M}_\odot}

% Software names
\newcommand{\boxlib}{\texttt{BoxLib}}
\newcommand{\castro}{\texttt{CASTRO}}
\newcommand{\microphysics}{\texttt{Microphysics}}
\newcommand{\wdmerger}{\texttt{wdmerger}}
\newcommand{\python}{\texttt{Python}}
\newcommand{\matplotlib}{\texttt{matplotlib}}
\newcommand{\yt}{\texttt{yt}}
\newcommand{\vode}{\texttt{VODE}}
\newcommand{\isoseven}{\texttt{iso7}}
\newcommand{\aproxthirteen}{\texttt{aprox13}}
\newcommand{\aproxnineteen}{\texttt{aprox19}}
\newcommand{\aproxtwentyone}{\texttt{aprox21}}

\title{\bf{White Dwarf Mergers on Adaptive Meshes}}

\vspace*{3\baselineskip}
\centerline{\bf{White Dwarf Mergers on Adaptive Meshes}}
\vspace*{1\baselineskip}
\centerline{A Dissertation presented}
\vspace*{1\baselineskip}
\centerline{by} 
\vspace*{1\baselineskip}
\centerline{\bf{Maximilian Peter Katz}}
\vspace*{1\baselineskip}
\centerline{to} 
\vspace*{1\baselineskip}
\centerline{The Graduate School}
\vspace*{1\baselineskip}
\centerline{in Partial Fulfillment of the}
\vspace*{1\baselineskip}
\centerline{Requirements}
\vspace*{1\baselineskip}
\centerline{for the Degree of}
\vspace*{1\baselineskip}
\centerline{\bf{Doctor of Philosophy}}
\vspace*{1\baselineskip}
\centerline{in}
\vspace*{1\baselineskip}
\centerline{\bf{Physics}}
\vspace*{2\baselineskip}
\centerline{Stony Brook University}
\vspace*{2\baselineskip}
\centerline{\bf{August 2016}}     

\newpage
\pagenumbering{roman}
\setcounter{page}{2}

\centerline{\bf{Stony Brook University}}
\vspace*{1\baselineskip}
\centerline{The Graduate School}
\vspace*{2\baselineskip}
\centerline{Maximilian Peter Katz}
\vspace*{2\baselineskip}
\centerline{We, the dissertation committe for the above candidate for the}
\vspace*{1\baselineskip}
\centerline{Doctor of Philosophy degree, hereby recommend}
\vspace*{1\baselineskip}
\centerline{acceptance of this dissertation}
\vspace*{2\baselineskip}
\centerline{\bf{Michael Zingale - Dissertation Advisor}}
\centerline{\bf{Associate Professor, Physics and Astronomy}}
\vspace*{2\baselineskip}
\centerline{\bf{Alan Calder - Chairperson of Defense}}
\centerline{\bf{Associate Professor, Physics and Astronomy}}
\vspace*{2\baselineskip}
\centerline{\bf{Joanna Kiryluk - Committee Member}}
\centerline{\bf{Assistant Professor, Physics and Astronomy}} 
\vspace*{2\baselineskip}
\centerline{\bf{Marat Khairoutdinov - External Committee Member}}
\centerline{\bf{Associate Professor, School of Marine and Atmospheric Sciences}}
\vspace*{2\baselineskip}
\centerline{This dissertation is accepted by the Graduate School}
\vspace*{3\baselineskip}
\centerline{Charles Taber}
\centerline{Dean of the Graduate School}

\newpage

\centerline{Abstract of the Dissertation}
\vspace*{1\baselineskip}
\centerline{\bf{White Dwarf Mergers on Adaptive Meshes}}
\vspace*{1\baselineskip}
\centerline{by}
\vspace*{1\baselineskip}
\centerline{\bf{Maximilian Peter Katz}}
\vspace*{1\baselineskip}
\centerline{\bf{Doctor of Philosophy}}
\vspace*{1\baselineskip}
\centerline{in}
\vspace*{1\baselineskip}
\centerline{\bf{Physics}}
\vspace*{1\baselineskip}
\centerline{Stony Brook University}
\vspace*{1\baselineskip}
\centerline{\bf{2016}}
\vspace*{2\baselineskip}
The mergers of binary white dwarf systems are potential progenitors of astrophysical
explosions such as Type Ia supernovae. These white dwarfs can merge either by orbital
decay through the emission of gravitational waves or by direct collisions as a result of
orbital perturbations. The coalescence of the stars may ignite nuclear fusion, resulting in
the destruction of both stars through a thermonuclear runaway and ensuing detonation.
The goal of this dissertation is to simulate binary white dwarf systems using the
techniques of computational fluid dynamics and therefore to understand what numerical
techniques are necessary to obtain accurate dynamical evolution of the system, as well as
to learn what conditions are necessary to enable a realistic detonation. For this purpose I
have used software that solves the relevant fluid equations, the Poisson equation for self-
gravity, and the systems governing nuclear reactions between atomic species. These
equations are modeled on a computational domain that uses the technique of adaptive
mesh refinement to have the highest spatial resolution in the areas of the domain that are
most sensitive to the need for accurate numerical evolution. I have identified that the
most important obstacles to accurate evolution are the numerical violation of
conservation of energy and angular momentum in the system, and the development of
numerically seeded thermonuclear detonations that do not bear resemblance to physically
correct detonations. I then developed methods for ameliorating these problems, and
determined what metrics can be used for judging whether a given white dwarf merger
simulation is trustworthy. This involved the development of a number of algorithmic
improvements to the simulation software, which I describe. Finally, I performed high-
resolution simulations of typical cases of white dwarf mergers and head-on collisions to
demonstrate the impacts of these choices. The results of these simulations and the
corresponding implications for white dwarf mergers as astrophysical explosion
progenitors are discussed.

\newpage
\centerline{\bf{Table of Contents}}
\renewcommand*\contentsname{}
\tableofcontents

\newpage
\centerline{\bf{List of Figures/Tables/Illustrations}}
\vspace*{4\baselineskip}

\listoffigures

\listoftables

\newpage
\centerline{\bf{List of Abbreviations}}
\vspace*{4\baselineskip}
Include this list if applicable.

\newpage
\centerline{\bf{Acknowledgements}}
\vspace*{4\baselineskip}
When I graduated from high school, the event that mattered most to me was 
not walking across the stage at commencement, but rather the end-of-year 
debate team dinner and awards ceremony. The group of people that had been
my friends on the debate team made my experience much more memorable and 
gave me a connection to the school that I would not otherwise have had.
While I have lost touch with many of the people I knew at the time, I am 
still very close friends with my friends from debate. Andrew, Chris D.,
Chris V., Chris K., Lucas, and the rest of our group of friends have 
what we have in large part because of that activity, and that is what 
really made my educational activities complete.

In the same way, while I have been honored to have the opportunity to 
earn a doctorate from Stony Brook University, what really has made my 
time here special is the group of people I regularly interacted with 
on the astronomy floor. My advisor, Mike Zingale, is of course brilliant 
and someone whose talent level is extraordinary enough that I hope to 
one day reach even a fraction of that level. But I did not come to Stony 
Brook for that reason; there are smart and talented people everywhere. 
I came here and put in the effort to complete this dissertation because 
Mike inspires me to do better and constantly challenge myself;
and, because he shares many of my values about what makes good science and 
how good software development for scientific computing should be done;
but most importantly, because he has the necessary sense of humor to work 
with me. The other half of the dynamic duo that brought me here is Alan 
Calder, and he too played a key role in my success. Alan never let me 
doubt that I was capable of producing great scientific work, and would 
always drop whatever he was doing to chat with me about my concerns about 
my work or about my future. Many people have academic advisors, but some 
of them go above and beyond the call of duty to be true mentors and friends,
and Alan does that every day, not just for me but for all of the students 
he works with. Doug Swesty also played a key role in the development of the 
project that I worked on and in giving me many pointers on how to proceed 
in a novel way when I felt stuck. Even when he couldn't solve my problem, 
he always could provide some interesting insight into it, because there 
are few sharper and more thoroughly knowledgeable people out there.

Then were the students on the floor who made every day a delight, and who
were always willing to chat when I walked over to bother them (in some cases
a little \textit{too} willing, as anyone who was present for one of my many
silly arguments with Rahul can attest). Rahul and Melissa L. are good friends 
who helped keep the astronomy group anchored, and I am glad I got the 
opportunity to enjoy their presence. Adam and Don have a wealth of knowledge 
about subjects that nicely complements my own and working with them helped my 
own productivity enormously. Melissa H. also helped my productivity by doing 
a project that I never had time for -- and she is also a dear friend. Mat 
was also a great office mate, the few times we were actually in the office 
together. Thanks for sharing death row with me, buddy. And to everyone else
-- Lupe, Tianqi, Taeho, and everyone else who spent any time in the grad 
suite -- thanks for always being there to entertain my thoughts.

In my speech at the aforementioned debate dinner, I mentioned that my 
parents were the largest factor in my success, and that's just as true 
now as it was back then. I am truly fortunate to have parents who without 
hesitation will always talk to me, or let me crash at their house on 
last minute notice, or help me out financially when I need it. I don't 
see them or talk to them nearly as much as they would probably like, 
but they are always in my thoughts and I know that I am always in theirs.
And aside from the people already mentioned, Andrea played the most significant 
role in shaping the adult that I have become, and I am grateful to have known 
her for many years and to call her my friend.

Finally, I would like to acknowledge the support of Ann Almgren and Weiqun 
Zhang at Lawrence Berkeley National Laboratory's Center for Computational 
Science and Engineering. They were core developers of the software that 
I used for the simulations in this work, but they were also wonderful resources 
for sounding off new ideas and figuring out how to implement many of the 
algorithms I considered over the years.

\textit{Please note that much of the material in this dissertation, especially in
\autoref{sec:introduction}, \autoref{sec:methodology}, \autoref{sec:verification},
and \autoref{sec:performance}, originally appeared in \cite{wdmergerI}. The content
from \autoref{sec:network}, \autoref{sec:burning} and \autoref{sec:collisions} contains draft content from
a paper to be submitted to the Astrophysical Journal entitled ``White Dwarf Mergers on
Adaptive Meshes. II. Collisions and Nuclear Burning'' (Max P. Katz, Michael Zingale,
Alan C. Calder, F. Douglas Swesty, Ann S. Almgren, Weiqun Zhang, Frank X. Timmes).
The content from \autoref{sec:mergers} contains draft content from a paper to be
submitted to the Astrophysical Journal entitled ``White Dwarf Mergers on Adaptive
Meshes. III. Inspiral and Coalescence'' (Max P. Katz, Michael Zingale, Alan C. Calder,
F. Douglas Swesty, Ann S. Almgren, Weiqun Zhang).}



\newpage
\pagenumbering{arabic}
\section{Introduction}
\label{sec:introduction}

Type Ia supernovae (SNe Ia) are among the most exciting
events to study in astrophysics. These bright, brief pulses of light
in the distant universe have led to a number of important discoveries
in recent years, including the discovery of the accelerated expansion
of the universe \citep{perlmutter1999,riess1998}. Their origin, though,
is shrouded in mystery. It has long been expected that these
events arise from the thermonuclear explosions of white dwarfs
\citep{hoyle-fowler:1960}, but the cause of these explosions is
uncertain. In particular, it is not clear what process causes the
temperatures in these white dwarfs (WDs) to become hot enough for explosive
burning of their constituent nuclei. The model favored initially by the
community was the single-degenerate (SD) model
\citep{whelan-iben:1973}. Accretion of material from a companion star
such as a red giant would cause the star to approach the Chandrasekhar
mass, and in doing so the temperature and density in the center would
become sufficient for thermonuclear fusion to proceed. In
recent years the focus has shifted to a number of alternative progenitor models. A
leading candidate for explaining at least some of these explosions is
the double-degenerate (DD) model, in which two white dwarfs merge and
the merged object reaches the conditions necessary for a thermonuclear
ignition \citep{ibentutukov:1984,webbink:1984}. Another is the double
detonation scenario, where accretion of material onto a
sub-Chandrasekhar mass white dwarf leads to a detonation inside the
accreted envelope, sending a compressional wave into the
core of the star that triggers a secondary detonation. A recent
review of the progenitor models can be found in
\citet{hillebrandt:2013}.

There are several observational reasons why double-degenerate systems
are a promising progenitor model for at least a substantial fraction
of normal SNe Ia. No conclusive evidence exists for a surviving
companion star of a SN Ia; this is naturally explained by the DD model
because both WDs are likely to be destroyed in the merger
process. Similarly, pre-explosion images of the SN Ia systems have
never clearly turned up a companion star, and in some cases a large
fraction of the parameter space for the nature of the companion star
is excluded. Additionally, not enough progenitor systems are seen for
the SD case to match the observed local SN Ia rate, whereas the number
of white dwarf binaries may be sufficient to account for this
rate. Finally, the DD model can naturally explain the fact that many
SNe Ia are observed to occur at very long delay times after the stars
were formed, since the progenitor systems only become active once both
stars have evolved off the main sequence. A thorough review of the
observational evidence about SNe Ia and further discussion of these
ideas can be found in \cite{maoz:2014}.

The first attempts to model the results of the merger process came in the
1980s. \cite{nomotoiben:1985} demonstrated that off-center carbon
ignition would occur in the more massive white dwarf as it accreted
mass near the Eddington rate from the less massive white dwarf
overflowing its Roche lobe. \cite{saionomoto:1985} tracked the
evolution of the flame and found that it propagated quiescently into
the center, converting the carbon-oxygen white dwarf into an
oxygen-neon-magnesium white dwarf. This would then be followed by
collapse into a neutron star---a result with significantly different
observational properties compared to a SN Ia. This scenario, termed
accretion-induced collapse, would be avoided only if the accretion
rate were well below the Eddington rate (see, e.g., \cite{fryer:1999}
for a discussion of the possible implications of the accretion-induced 
collapse scenario). \cite{tutukov-yungelson:1979}
observed that the collapse could be avoided if the mass loss from the secondary
was higher than the Eddington rate and thus the accreted material
formed an accretion disc, which might rain down on the primary more
slowly. The main finding was that double degenerate systems would not
obviously lead to Type Ia supernovae.

Three-dimensional simulations of merging double degenerate systems were 
first performed by \citet{benz:1990}, who used the smoothed particle
hydrodynamics (SPH) method to simulate the merger process. This was 
followed later by a number of authors 
\citep{rasio-shapiro:1995,segretain:1997,guerrero:2004,yoon:2007,loren-aguilar:2009,raskin:2012}.
The main finding of these early 3D SPH simulations was that if the 
lower-mass star (generally called the ``secondary'') was
close enough to the more massive star (the ``primary'') to begin mass
transfer on a dynamical time scale, the secondary completely disrupted
and formed a hot envelope around the primary, with a
centrifugally-supported accretion disk surrounding the core and
envelope. Carbon fusion might commence in the disk, but not at a 
high enough rate to generate a nuclear detonation. \cite{mochkovitch-livio:1990} 
and \cite{livio:2000}  also observed that turbulent viscosity in this disk 
would be sufficiently large for angular momentum to be removed from the 
disk at a rate high enough to generate the troublesome accretion 
timescales discussed by \cite{tutukov-yungelson:1979} and mentioned above. Based on this
evidence, the review of \cite{hillebrandtniemeyer2000} argued that the
model was only viable if the accretion-induced collapse problem could
be avoided. Later work by \cite{shen:2012} and \cite{schwab:2012} used
a more detailed treatment of the viscous transport in the outer
regions of the remnant and found that viscous dissipation in the centrifugally
supported envelope would substantially heat up the envelope on a  
viscous timescale, but their simulations still led to off-center carbon
burning. \cite{vankerkwijk:2010} argued that equal-mass mergers would
lead to the conditions necessary for carbon detonation in the center
of the merged object, but \cite{shen:2012} also questioned this for
reasons related to how viscous transport would convert rotational
motion into pressure support. \cite{zhu:2013} followed this with an
expanded parameter space study and argued that many of their
carbon-oxygen systems had the potential to detonate. The study of the
long-term evolution of the remnants is thus still an open subject of
research.

A recent shift in perspective on this problem started around 2010.
\cite{pakmor:2010} used the SPH method to study the merger of 
equal-mass ($0.9\ \msolar$) carbon-oxygen white dwarfs and found 
that a hotspot was generated near the surface of the primary 
white dwarf. They argued that this region had a temperature 
and density sufficient to trigger a thermonuclear
detonation. They inserted a detonation which propagated throughout 
the system. They found that the result would observationally 
appear as a subluminous Type Ia supernova. This was the first time 
a DD simulation successfully reproduced at least some characteristics of a SN
Ia. \cite{pakmor:2011} tried a few different mass combinations and
found empirically that this would hold as long as the secondary was at
least 80\% as massive as the primary. These events, where the merger
process resulted in the detonation of the system during the merger
coalescence---avoiding the much longer time-scale evolution---were
termed ``violent'' mergers.

Around the same time, however, \cite{guillochon:2010} and
\cite{dan:2011} pointed out that the previously mentioned simulations 
generally shared a significant drawback, which was that their initial conditions
were not carefully constructed. \cite{motl:2002}, \cite{dsouza:2006},
and \cite{motl:2007} (the first three-dimensional mesh-based
simulations of mass transfer in binary white dwarf systems) pioneered
the study of the long-term dynamical evolution of binary
white dwarf systems after constructing equilibrium initial
conditions. Earlier work placed the stars too close together 
and ignored the effects of tidal forces that change the shape of the 
secondary, leading to the merger
happening artificially too quickly \citep{fryer:2008}. When the initial conditions are
constructed in equilibrium, the system can be stable for tens of
orbital periods, substantially changing the character of the mass
transfer phase. One limitation of this series of studies is
that the authors used a polytropic equation of state and thus could
not consider nuclear reactions. \cite{guillochon:2010} and
\cite{dan:2011} improved on this using a realistic equation of state,
a nuclear reaction network, and a similar approach to the equilibrium
initial conditions, and found substantial agreement with the idea that
mass transfer occurs in a stable manner over tens of orbital
periods. They also found that, assuming the material accreted onto the
surface of the primary was primarily helium, explosive surface
detonations would occur as a result of accretion stream instabilities
during the mass transfer phase prior to the full merger. This could
trigger a double-detonation explosion and thus perhaps a SN Ia.

The latest violent merger developments have resulted in some possible areas of convergence.
\cite{pakmor:2012} performed a merger scenario
with a $1.1\ \msolar$ and $0.9\ \msolar$ setup, with better treatment
of the initial conditions, and indeed found that the merger process
happened over more than ten orbits. Nevertheless, they still determined
that a carbon-oxygen detonation would occur, in line with their
earlier results. \cite{moll:2014} and \cite{kashyap:2015} were also 
able to find a detonation in similarly massive systems. Notably,
the detonation occurred self-consistently and did not need to be  
intentionally triggered using an external source term.
\cite{dan:2012} and \cite{dan:2014} performed a large sweep 
of the parameter space for merger pairs and
found that pure carbon-oxygen systems would generally not lead to
detonations (and thus be violent mergers) except for the most massive
systems. They did find that for systems with WDs containing helium, many
would detonate and potentially lead to SNe Ia, either through the
aforementioned instabilities in the accretion stream, or during the
contact phase, similar to the violent carbon-oxygen WD
mergers. \cite{sato:2015} also examined the parameter space and
came to a similar conclusion for massive carbon-oxygen WD systems
(and also looked at the possibility of detonations after the
coalescence had completed), while \cite{tanikawa:2015} discussed
the plausibility of helium detonations in the massive binary case.
\cite{pakmor:2013} added a thin helium shell on their primary
white dwarf, and found that this robustly led to a detonation of the
white dwarf. For now there is preliminary support for the hypothesis
that systems with helium shells (or helium WDs), and very massive carbon-oxygen binaries,
could robustly lead to events resembling SNe Ia.

Given the considerable research into the double degenerate problem 
described above, why is another approach using a different simulation
code warranted? First and foremost, reproducibility of the results
across simulation codes and algorithms is important for gauging
confidence in this result. Most of the existing results that study 
the viability of double degenerate systems as progenitors for
Type Ia supernovae (that is, including a realistic 
equation of state and nuclear reactions) have
used the SPH method. SPH codes have a number of features which do aid
them in the study of these systems, such as conservation of
angular momentum to machine precision when there are no source terms
such as gravity (and conservation proportional to the level of
tolerance of error in the gravity solver when gravity is used).
A drawback relates to the fact that whether a prompt detonation
in a merger happens depends in detail on the nature of the
gas at the interface between the two stars, which is at much lower
density than the rest of the stellar material. The SPH codes for these
simulations generally all use
uniform mass particles, so their effective resolution is
\textit{lowest} at the stellar surface. In contrast, a code
with adaptive mesh refinement can zoom in on the regions where
hotspots will develop, while also maintaining high enough resolution
in the high-density regions to adequately capture the large-scale mass
transfer dynamics. There are also outstanding questions of
convergence in SPH (e.g.\ \citealt{zhu-SPH:2014}) and whether the method
correctly captures fluid instabilities. This is an important question
for white dwarf mergers because of the likely importance small-scale
instabilities will have on the evolution of the low-density gas at the
primary's surface. The pioneering work of \cite{agertz:2007} compared
grid and SPH codes and found some important differences. Most relevant
for this discussion is that the SPH codes could not adequately handle
mixing from the Kelvin-Helmholtz instability in the test they
propose. As pointed out by \cite{price:2008}, this is not a result of
SPH being inherently unable to model this instability, but instead it
is attributed to the fact that the standard SPH evolution equations do
not have a mechanism for capturing discontinuities in internal
energy. \citeauthor{price:2008} showed that the addition of an
artificial thermal conductivity can dramatically improve the ability
of the SPH codes to exhibit this instability. There have since been a
number of other papers discussing this issue, but to our knowledge
none of these improvements have yet been incorporated into an SPH
model of a WD merger. Another reason for caution is that other than the
most recent results of \cite{kashyap:2015}, no white dwarf merger simulation has self-consistently
resulted in a thermonuclear detonation. Reproducibility of the detonation 
through numerical simulation is critical for building 
confidence in this progenitor model.

This paper is the first in a series designed to address these
outstanding theoretical issues for white dwarf mergers. This work 
discusses the verification of our hydrodynamics code for simulating
these events. Later efforts will look at the initial conditions of the
system, the robustness with which a hotspot is found from which a
detonation could occur, and the importance of the initial white dwarf
models, which should be more sophisticated than simple carbon-oxygen
mixtures and in principle should use results from modern stellar
evolution calculations. \autoref{sec:methodology}
describes our code and why it can provide useful results compared to
other methodologies used for this problem. 
\autoref{sec:software} describes the method we use for setting up a
binary white dwarf simulation. \autoref{sec:verification} discusses a few
test problems that we use to verify that our code accurately
solves the equations of fluid dynamics. \autoref{sec:performance}
demonstrates that the software scales well for supercomputer
applications. In \autoref{sec:collisions} we discuss our results for
collisions of white dwarfs, and in \autoref{sec:mergers} we discuss
our results for mergers of white dwarsf. Finally, \autoref{sec:conclusion}
recaps what we have shown and highlights some of the future work we
plan to do.



\newpage
\section{Numerical Methodology}
\label{sec:methodology}

To study the white dwarf merger problem, we use the mesh-based
hydrodynamics code \castro\footnote{\castro\ can be obtained at \url{https://github.com/BoxLib-Codes/Castro}.} \citep{castro}.
\castro\ solves the Euler
equations, along with the inclusion of optional modules for gravity,
nuclear reactions and thermodynamics. \castro\ is based on the \boxlib
\footnote{\boxlib\ can be obtained at \url{https://github.com/BoxLib-Codes/BoxLib}.}
adaptive-mesh refinement (AMR) framework \citep{rendleman:2000}, which
represents fluid data on a hierarchical mesh where regions of interest have higher
spatial resolution. \castro\ is highly parallel and is designed for
large-scale use on modern supercomputers; see 
\autoref{sec:performance} for information on how \castro\ performs for our
problem. The next few subsections describe our approach to each of the
physics components used in this work. We direct the reader to the
original code paper for a full description of \castro's approach to
solving the equations of hydrodynamics. In this work, we report mainly
on the changes we have made to the code since its original release,
for the purpose of approaching this problem.

\subsection{Hydrodynamics}
\label{sec:hydrodynamics}

The Euler equations for hydrodynamics (in the absence of source terms) in conservative form are: 
\begin{align}
  \frac{\partial \rho}{\partial t} &= -\bm{\nabla} \cdot (\rho \mathbf{u}) \label{eq:euler_density}\\
  \frac{\partial \rho \mathbf{u}}{\partial t} &= -\bm{\nabla} \cdot (\rho \mathbf{u}\mathbf{u}) - \bm{\nabla}p \label{eq:euler_momentum}\\
  \frac{\partial \rho E}{\partial t} &= -\bm{\nabla}\cdot(\rho\mathbf{u}E + p\mathbf{u}). \label{eq:euler_energy}
\end{align}
Here $\rho$ is the mass density, $\mathbf{u} = (u, v, w)$ is the fluid velocity
vector, $p$ is the pressure, and $E = \mathbf{u}^2 / 2 + e$ is the
total specific energy, where $e$ is the internal (thermal) specific
energy (energy per unit mass).

We use the unsplit piecewise-parabolic method (PPM) solver in \castro\
to advance the hydrodynamics system in time \citep{ppmunsplit}.  A
number of changes were made to the solver, which are detailed in Appendix A
of \cite{wdmergerI}.
These changes bring the algorithm more in line with that of
\cite{ppm}. \castro\ as originally released featured a slightly modified
version of the higher resolution limiters of
\cite{colella-sekora:2008}, which can be used in the code by setting 
\texttt{castro.ppm\_type = 2} in the inputs file (the inputs file is
a set of code parameters accessed at runtime to determine the algorithms
used in the simulation). The advantage of this limiter is that
it preserves physical extrema rather than clipping them off as in the
original approach of \cite{ppm}. Despite the advantages of this limiter 
we have found it to be unsatisfactory for our problem. There are many regions in our
problem with large density gradients (such as the interface between
the star's atmosphere and the ambient gas outside of it) and in these
regions the algorithm can yield negative densities. This often results
from the limiters interpreting these gradients as being true
minima. As a result, we use the original limiter, which is strictly
monotonicity preserving in the parabolic profiles it generates; this
is activated with \texttt{castro.ppm\_type = 1} in the inputs file.

A related issue that required a code improvement is that in cases of
large density gradients such as the edge of a star, it is possible to
generate negative densities in zones even with the more strongly
limited PPM. This can occur if a region of large density is moving
away from an ambient zone at relatively large speeds; then the net
density flux in the ambient zones can be large enough to unphysically
drag the density below zero. In practice, this occurs at the
trailing edge of a star that is moving across a grid. In such a
situation, there are two main approaches one could take: either
explicitly introduce a positivity-guaranteeing diffusive flux, or
reset the properties of the affected zone. We choose the latter
approach. Even though it is non-conservative, it preserves a
characteristic we value, which is to keep the edge of the stars
relatively sharp, as they physically should be. Since the mass of the
affected zones is typically already fairly low, this should not
seriously affect the dynamics or the energy conservation properties of our
simulation. Our strategy for a reset is as follows: when the density of 
a zone is below a pre-determined density floor (which is typically 
$10^{-5}\ \text{g cm}^{-3}$ for our stellar simulations), we look
at all adjacent zones and find the zone with the highest density.
If it is above the density floor, then we set the field values 
(density, momentum, energy, and temperature) of the
reset zone to be equal to the field values of this 
adjacent zone. If no adjacent zone reaches the density floor, then
the zone is set to the density floor, and given a temperature equal 
to the temperature floor for our simulations (which is typically 
$10^{5}\ \text{K}$ for our stellar simulations). We then recompute 
the thermodynamics to be consistent with these values. The 
velocity of the zone is set to zero. This latter approach only
occurs in very rare situations, and is there as a last resort.

\castro's approach to adaptive mesh refinement, based on its underlying
\boxlib\ framework, is to refine zones based on certain user-specified
criteria that tag regions of interest for higher spatial
resolution. Data is represented on one of a number of AMR levels,
where each level corresponds to a set of zones at the same resolution,
which covers a subset of the domain covered by the level immediately
below it. We typically call the level 0 grid the \textit{coarse} grid,
which has the lowest spatial resolution. Each finer, higher-level grid
has a higher resolution than the grid below it by some integer factor
$N$, which is restricted to be $N = 2\ \text{or}\ 4$ in \castro. The
zones are strictly contained within the rectangular extent of the
underlying coarser zones (at present, in 3D the code is restricted to representing
only Cartesian geometries with uniform spacing in each dimension). For the time
evolution of the AMR system we use subcycling, where each AMR level is
advanced at a different timestep and a correction step is applied at
the end to synchronize the various levels. The number of
subcycled timesteps is equal to the jump in refinement between levels,
so for example on a grid with three levels and two jumps of four in
refinement, the level 2 zones have 16 times higher spatial
resolution than the coarse grid and there are 16 level 2 timesteps
per level 0 timestep.

The boundary conditions on the hyperbolic system are simply
zero-gradient zones that allow material to flow directly out of the
domain. Using AMR, we make the coarse grid large enough that the
boundaries are relatively far from the region of interest. This
ensures that any boundary effects do not pollute the inner region
where the stars will eventually make contact.  We further make the
restriction that refined grids cannot reach the domain boundary.

%TODO: describe hybrid momentum
%TODO: discuss source term predictor?
%TODO: discuss retries?

\subsection{Equation of State}
\label{sec:eos}

The equation of state (EOS) for our simulations is the Helmholtz EOS
\citep{timmes-swesty:2000}. This models an electron-positron gas of
arbitrary relativity and degeneracy over a wide range of temperatures
and densities. Thermodynamic quantities are calculated as derivatives
of the Helmholtz free energy, and the values are interpolated from a
table. The natural variables of the Helmholtz free energy are
temperature and density, and calling the EOS is simplest in this
form. In hydrodynamics we often have the density and
internal energy as independent variables, and we want to obtain the
temperature, pressure, and other quantities. To do this, we employ a
Newton-Raphson iteration over the temperature (given some sufficient
starting guess) until we find the temperature that corresponds to the
desired internal energy. Sometimes this process fails to converge and
the iterative value approaches zero. In these cases we employ a
``floor'' that limits how low the temperature can go (typically 
$10^5$ K). There is a choice here how to proceed: we can either
assign this floor value to the temperature and let that zone be
thermodynamically inconsistent (the original behavior in \castro), or
we can adjust the internal energy to be thermodynamically consistent
with the temperature, at the cost of violating energy conservation. We
have found in some test problems of strong one-dimensional shocks that reach 
the temperature floor that the latter yields more accurate results. 
However, allowing the equation of state call to update the 
internal energy can actually result in significant changes to the 
total energy of the system over long periods of time, 
due not just to resets in low-density zones but also to small 
inconsistencies between the energy given to the EOS and the energy 
that is consistent with the returned temperature. These inconsistencies
are dependent on the tolerance of the Newton-Raphson iterative solve.
While this error tolerance is typically very small in an individual zone (a relative 
difference of $10^{-8}$ by default in \castro), over time and given 
a large number of zones, this can result in a significant energy 
drift. This is a serious enough problem that we opt for the energy 
conserving approach for our simulations.

\subsection{Gravity}
\label{sec:gravity}

We solve the Poisson equation for self-gravity for our problem,
\begin{equation}
  \nabla^2 \Phi(\mathbf{x}) = 4\pi G\, \rho(\mathbf{x}),
\end{equation}
where $\Phi$ is the gravitational potential, $G$ is the gravitational
constant, and $\rho$ is the mass density.\footnote{In the \castro\ code, the 
right-hand side is negated and therefore $\Phi$ is positive. We use the 
sign convention that is typical for astrophysics in this paper. 
When $\Phi$ appears in the code it is negated to compensate for this.} 
The solution of this equation in \castro\ is described in \cite{castro}, and
consists of both level and composite solves, and (optionally) a final
synchronization at the end. We do not enable this final synchronization
for the merger simulations, because the grid boundaries never lie in
regions of high density, so the change in the potential due to the correction
at coarse--fine interface is always negligible.

\subsubsection{Coupling to Hydrodynamics}\label{sec:gravity_hydro_coupling}

The effect of gravity on the hydrodynamical evolution is typically
incorporated by the use of a source term for the momentum and energy
equations. In a finite volume methodology, the momentum source term 
often appears in integral form as
\begin{equation}
  \left.\frac{\partial (\rho \mathbf{u})}{\partial t}\right|_{\text{grav}} = \frac{1}{\Delta V} \int \rho \mathbf{g}\, dV
\end{equation}
and for the energy source term it is
\begin{equation}
  \left.\frac{\partial (\rho E)}{\partial t}\right|_{\text{grav}} = \frac{1}{\Delta V} \int \rho \mathbf{u}\cdot\mathbf{g}\, dV \label{eq:cell_center_gravity_source}.
\end{equation}
Here $\Delta V$ is the cell's volume.
In most hydrodynamics codes these are discretized as $\rho\,
\mathbf{g}$ and $\rho\, \mathbf{u}\,\cdot\mathbf{g}$, respectively, 
where $\rho$, $\mathbf{u}$, and $\mathbf{g}$ 
are evaluated at the zone center. 

There are two ways that these source terms enter the system evolution. 
First, during the hydrodynamics update, we alter the edge states that enter
into the determination of the fluxes. (This only applies for the momentum source term;
the gravitational force does not directly do work on the internal energy, which is used 
to infer the pressure.) To second order in space and time, 
this can be done using the cell-centered
source term evaluated at time-level $n$. We choose a more accurate approach, 
which is also second order, of characteristic tracing
under the source term; the details of this are described in Appendix A of \cite{wdmergerI}.
Second, after the hydrodynamics step, we add the time-centered source terms
to the state. First we describe how we do this for the momentum,
and then we describe our approach for the energy. This discussion is somewhat detailed.
We believe that the attention is necessary because of the importance of accuracy
in the gravitational source terms for our problem. The stability of the white dwarf binary
system is dependent in large part upon accurate coupling of the hydrodynamics and gravity;
an error in this approach could lead to, for example, a spurious mass transfer episode
that might lead us to very different conclusions about the long term stability of such a system.
Such considerations are generally unimportant for spherically-symmetric single star calculations,
but are of the utmost importance in a simulation where the global gravitational field can change 
quite significantly over the course of the simulation.

In a system with self-gravity, total momentum is conserved if the spatial domain
includes all of the mass of the system. This must be the 
case because each mass element exerts an equal and opposite gravitational force 
on every other mass element. However, the standard approach does not necessarily
guarantee that momentum is conserved numerically. We cannot represent a vacuum state 
in our code, so there is a small but non-zero density on the edge of the grid. 
This allows momentum to leak out of the domain even if the gravitational source term 
is written in an explicitly conservative manner. To see this, one can use the Poisson equation to write the 
density in terms of the potential and then consider its spatial discretization. For simplicity,
we consider one spatial dimension and a uniform discretization. Analogous results 
may be readily obtained for the non-uniform case.
\begin{align}
  -\rho_{i}  \frac{d\Phi_{i}}{dx} &= -\frac{1}{4\pi G} \frac{d^2\Phi_i}{dx^2} \frac{d \Phi_i}{dx} \notag \\
  &= -\frac{1}{4\pi G} \left[\frac{\Phi_{i-1} - 2 \Phi_{i} + \Phi_{i+1}}{\Delta x^2}\right] \left[ \frac{\Phi_{i+1} - \Phi_{i-1}}{2\Delta x} \right] \notag \\
  &= -\frac{1}{8\pi G \Delta x^3} \left[ \Phi_{i+1}^2 - \Phi_{i-1}^2 - 2\Phi_i\left(\Phi_{i+1} - \Phi_{i-1}\right) \right] \label{eq:momentum_discretization}
\end{align}
It is easy to verify that adding the source terms for the current zone and the two zones 
to the left and right results in complete cancellation of the source terms.
The catch is that if the potential if non-zero outside of the domain, then there will be
momentum lost or gained from the grid, which will be encapsulated in the ghost cells
just outside the domain. In addition, when we replace the Laplacian above by the full
three-dimensional stencil including the $y$ and $z$ derivatives, depending on the
discretization these may not be cancelled at all. This latter problem can be resolved by
writing the momentum update in an explicitly conservative way.

\citet[Chapter 4]{shu:1992} observes that it is possible to describe the source term 
for the momentum equation by taking the divergence of a gravitational stress tensor,
\begin{equation}
  G_{ij} = -\frac{1}{4\pi G}\left(g_i g_j - \frac{1}{2}|\mathbf{g}|^2\delta_{ij}\right).
\end{equation}
The momentum equations are then written explicitly in conservative form.
The flux at any zone boundary is added to one cell and
subtracted from another, so that the total momentum in the domain interior stays constant to
within numerical roundoff error. This result can be derived by analytically recasting 
\autoref{eq:momentum_discretization}. In the continuum limit, the two momentum
formulations are identical. Thus the latter has been advocated by, for example, 
\cite{jiang:2013} for the ATHENA code. A significant limitation to this approach is that in a finite discretization 
the divergence of the gravitational acceleration is no longer guaranteed to equal
the zone density. In particular, we find that the mixing of the gravitational accleration components
means that the truncation error in the gravitational field can lead to large errors
that imply a density much different than the zone's actual density. This is especially
problematic in a simulation with a low-density ambient medium, where even a small error 
in the momentum update can lead to large changes in a zone's momentum. By continuing to explicitly
use the cell density in the momentum update, we can avoid this possibility: the size of the update
will always be suitably small if the zone's density is small. Thus for our simulations
we continue to use the standard source term for the momentum.

Time centering of this source term is done in \castro\ using a predictor-corrector approach.
At the start of a coarse grid timestep, we solve the gravitational potential for the density $\rho^n$.
We then add to the momenta a prediction of the source term that is first-order accurate in time, 
$\Delta t\, \rho^n\, \mathbf{g}^n$. After the hydrodynamics update, we recalculate
the gravitational potential based on the new density, $\rho^{n+1}$, and then add 
$-(\Delta t/2) \rho^n \mathbf{g}^n + (\Delta t/2) \rho^{n+1} \mathbf{g}^{n+1}$ to the momenta.

For the energy equation, the central challenge is to write down a form of the 
discretized energy equation that explicitly conserves total energy when 
coupled to gravity. When gravity is included, the conserved total energy
over the entire domain is
\begin{equation}
  \int \rho E_{\text{tot}}\, dV = \int dV \left(\rho E + \frac{1}{2}\rho\Phi\right), \label{eq:total_energy_gravity}
\end{equation}
where $\rho E$ is the total gas energy from the pure hydrodynamics equation. 
The factor of 1/2 in the gravitational energy is necessary for simulations with
self-gravity to prevent double-counting of interactions (since in dynamical evolution
the relevant gravitational potential energy is $\rho \Phi$ and the gravitational force
is $\rho \mathbf{g}$). Historically many simulation codes with gravity have not used
a conservative formulation of the energy equation, but it is straightforward to do so.
Our approach, and the discussion that follows, is based on that of \cite{arepo}.

Conservation of total energy requires that a change in gravitational energy is compensated
for by a change in gas energy, and that energy changes due to mass transfer are explicitly and 
exactly tracked. Suppose that we have some fluid mass $\Delta M_{i+1/2} = \Delta \rho_{i+1/2} \Delta V$ leave the zone
with index $i$ and enter the zone with index $i+1$. The subscript indicates that the mass change is
occurring at the interface between the two zones, at index $i+1/2$. The work done by the gravitational
force on the gas is $\Delta (\rho E) = W = \int F dx = (\Delta M_{i+1/2}\ g_{i+1/2}) (\Delta x / 2)$,
where $g_{i+1/2}$ is the gravitational acceleration at the interface. The second term in parentheses
is just the distance from the zone center to the zone edge: once the mass leaves the zone edge, it no longer
needs to be tracked. To second order, $g_{i+1/2} = -(\Phi_{i+1} - \Phi_{i}) / \Delta x$, and also to second order the potential
at the interface is given by $\Phi_{i+1/2} = (\Phi_{i+1} + \Phi_i) / 2$, so we can equivalently view the work done
as $W = -\Delta M_{i+1/2} (\Phi_{i+1/2} - \Phi_i)$. Physically, this is just the negative of the gravitational
potential energy change as the fluid is pushed from the cell center potential to the cell edge potential,
exactly as the work-energy theorem implies. 

Now, in a hydrodynamics code, mass changes correspond to hydrodynamic fluxes. In particular,
the continuity equation tells us that the mass flux $F_\rho = \rho^{n+1/2}_{i+1/2} v^{n+1/2}_{i+1/2}$ yields
an integrated mass motion through the interface $i+1/2$ over a timestep $\Delta t$ of:
\begin{equation}
  \Delta \rho_{i+1/2} = \frac{\Delta t}{\Delta V} \left(\rho^{n+1/2}_{i+1/2} v^{n+1/2}_{i+1/2} dA\right).
\end{equation}
Note that here $v_{i+1/2}$ is the component of the velocity perpendicular to the zone face, whose
area is $dA$.

Finally, then, we write the update in a zone for the total energy that conserves $(\rho E_{\text{tot}})$ as:
\begin{equation}
  \Delta (\rho E) = -\frac{1}{2}\sum_{f} \Delta \rho_{f} (\Phi_{f+1/2} - \Phi_{f-1/2}),\label{eq:grav_energy_conservation_update}
\end{equation}
where the sum is over the cell faces with indices $f$ and the indices $f+1/2$ and $f-1/2$ refer to 
the zone centers immediately to the left and right in the direction perpendicular to the face.
As long as we record the hydrodynamical fluxes through the zone faces after coming out of the hydrodynamics step, 
this algorithm is able to conserve the total energy completely (except for any energy loss or gain through 
physical domain boundaries). In order for the method to be second-order accurate in time, 
we need to use a time-centered $\Phi$ (which can be computed by averaging the time-level $n$ and $n+1$ potentials;
we already have the latter because \castro\ re-computes the potential at the new time after the hydrodynamics step,
and we can apply this energy at the end of the timestep). Note that of course the hydrodynamical
flux is already second-order accurate in time. We observe also that in practice we will not obtain 
conservation of energy to machine precision even in the absence of open domain boundaries. The 
method itself is conservative if it is time-centered and correctly evaluates the energy change 
on cell faces. This was demonstrated empirically by \cite{jiang:2013} and is obvious in the case of a
fixed external potential; it is not as obvious in the case of the gravitational self-potential, which
changes in response to changes in the mass distribution, so we give a short proof of this in
\autoref{app:gravity}. However, in practice there is a non-zero numerical tolerance associated 
with the Poisson gravity solver (in our case, the multigrid method) that results in a non-zero error 
in the calculation of the gravitational potential. This results in a very small deviation from perfect 
conservation. It is not usually larger than the other effects which result in energy non-conservation 
for our simulations, such as resetting the state of zones that acquire a negative internal energy, and 
in principle if desired it can be made smaller by using stricter tolerance levels on the gravity solve.

In passing, we hope to clear up a spot of potential confusion, that we feel is unclear in other papers
on this subject: the factor of $1/2$ that appears in \autoref{eq:grav_energy_conservation_update} 
has nothing to do with the factor of $1/2$ that appears in the statement of conservation of total energy, 
\autoref{eq:total_energy_gravity}. The former comes simply from the fact that the energy change is 
evaluated using the mass motion through a distance of half of the zone width. The latter is needed 
to ensure that these local changes in energy are not double-counted when doing a global integral, 
since the gravitational potential is self-generated. \autoref{eq:grav_energy_conservation_update} 
applies to any conservative potential $\Phi$, and we use this to our advantage for the 
rotation forces in \autoref{sec:rotation}.

As observed by \cite{arepo}, this method is more accurate than the more common (non-conservative) approach
of evaluating the change in gas energy using the work done $(\mathbf{v} \cdot \rho \mathbf{g})$
by the gravitational force at the cell center. Analytically this form expresses the same core idea as
\autoref{eq:grav_energy_conservation_update} via the work-energy theorem, but a major flaw is that
it evaluates the energy change at the cell center when in fact the mass transfer is happening at
the cell edges. This can result in a significant leaking of energy throughout the course of the
evolution, dramatically affecting the course of the evolution. The standard approach is therefore
unacceptable in the case of a problem like white dwarf mergers, and the fix to this energy
leaking---evaluating the energy transfer at the six zone faces instead of the single zone
center---adds only a very minor cost in terms of code complexity and computational time.

Another approach to conserving total energy recently taken in the literature is to evolve an 
equation for the total energy $(\rho E_{\text{tot}})$; see \cite{jiang:2013} (see also 
\cite{arepo}, Section 5.3). That is, one can replace the gas energy equation with a total energy equation, 
and then the energy flux includes a term corresponding to the flux of gravitational potential energy. We 
avoid this approach for our problem because there are regions on the computational domain where the total 
energy is dominated by potential energy (especially the low-density regions near the edge of the white dwarfs),
and the gas energy can only be retrieved by first subtracting $-\rho \Phi/2$ from the total energy. Like 
\cite{arepo}, we find that this can result in some serious errors due to numerical discretization, yielding 
unphysical energies or temperatures. We observe also that the implementation of \cite{jiang:2013} 
has terms in the gravitational flux that are not proportional to $\rho$, and so can lead to the 
same troubles that plague the tensor-based formalism for the momentum equation, where small errors 
in the discretization of the gravitational potential can lead to very large changes in the energy of the gas.

\subsubsection{Boundary Conditions}
\label{sec:gravity_boundary_conditions}

Analytical solutions to the Poisson equation customarily assume that the
potential vanishes at large distances from the region of non-zero
density. On a finite computational domain, however, it is usually not
possible to have the edges of the domain be far enough away that the
potential can be taken to be zero there. Solving the Poisson equation
therefore requires knowledge of the values of the potential on the
edges of the computational domain. In principle, the boundary values can be computed
by doing a direct sum over the mass distribution inside the domain,
where the mass in each zone is treated as a point source:
\begin{equation}
  \Phi_{{lmn}} = -\sum_{{i, j, k}} \frac{G \rho_{{ijk}}}{|\mathbf{x}_{{lmn}} - \mathbf{x}_{{ijk}}|}\, \Delta V_{{ijk}}.\label{eq:direct_sum}
\end{equation}
Here $(i, j, k)$ are the indices of cells inside the domain, and $(l,m, n)$ 
are the indices of ghost zones outside the domain where the boundary values for the potential is specified\footnote{In \castro\ we actually
specify the potential on cell edges, not on cell centers, but the idea is the same, and we use the location of
the cell edge in computing the distance to each zone in the domain.}. $\Delta V$ is the volume of the
zone. If there are $N$ zones per spatial dimension, then there are
$6 N^2$ boundary zones, and each boundary zone requires a sum over
$N^3$ zones, so the direct computation of the boundary conditions
scales as $\mathcal{O}(N^5)$.  This method is expensive enough that it is not used
for hydrodynamics simulations (though it is useful for comparison to
approximate solutions, so we have implemented it as an option in \castro).

In a typical simulation we place the boundaries of the domain far
enough away from the region containing most of the mass that some
method of approximation to this direct summation is justified. Many
approaches exist in the literature. The original release of \castro\
featured the crudest possible approximation: a monopole prescription,
where the boundary values were computed by summing up all the mass on
the domain and treating it as a point source at the domain
center. This is correct only for a spherically symmetric mass
distribution, and therefore is best suited for problems like
single-star Type Ia supernova simulations (e.g.$\ $\cite{malone:2014})
that employ self-gravity. For a problem like that of a binary star system
with significant departures from spherical symmetry, this assumption
fails to produce accurate boundary values, which we find in \autoref{sec:kepler}
results in a significant drift of the center of the mass of the system over time.

The most natural extension of the monopole prescription is to include
higher-order multipole moments. If the entire mass distribution is
enclosed, then the potential can be expanded in a series of spherical
harmonics $Y_{lm}(\theta,\phi)$ (where $\theta \in [0, \pi]$ is the usual polar angle
with respect to the $z$ axis and $\phi \in [0, 2\pi)$ is the usual azimuthal angle with
respect to the positive $x$ axis):
\begin{equation}
  \Phi(\mathbf{x}) = -\sum_{l=0}^{\infty}\sum_{m=-l}^{l} \frac{4\pi}{2l + 1} q_{lm} \frac{Y_{lm}(\theta,\phi)}{r^{l+1}}, \label{eq:spherical_harmonic_expansion}
\end{equation}
where $q_{lm}$ are the so-called multipole moments. The origin of the
coordinate system is taken to be the center of the computational
domain, and $r$ is the distance to the origin. The multipole moments
can be calculated by expanding the Green's function for the Poisson
equation as a series of spherical harmonics. After some algebraic
simplification of \autoref{eq:spherical_harmonic_expansion}, 
the potential outside of the mass distribution can be written as:
\begin{align}
  \Phi(\mathbf{x}) &= -\sum_{l=0}^{\infty} \left\{Q_l^{(0)} \frac{P_l(\text{cos}\, \theta)}{r^{l+1}} \right. \notag \\
    &+ \left. \sum_{m = 1}^{l}\left[ Q_{lm}^{(C)}\, \text{cos}(m\phi) + Q_{lm}^{(S)}\, \text{sin}(m\phi)\right] \frac{P_{l}^{m}(\text{cos}\, \theta)}{r^{l+1}} \right\}.\label{eq:multipole_potential}
\end{align}
$P_l(x)$ are the Legendre polynomials and $P_{l}^{m}(x)$ are the associated Legendre polynomials.
$Q_l^{(0)}$ and $Q_{lm}^{(C,S)}$ are variants of the multipole moments that involve integrals of
$P_l$ and $P_l^m$, respectively, over the computational domain; their definition is given in \autoref{app:multipole}.

This approach becomes computationally feasible when we cut off the
outer summation in \autoref{eq:multipole_potential} at some finite
value of $l_{\text{max}}$. If it is of sufficiently high order, we
will accurately capture the distribution of mass on the grid. In
practice we first evaluate the discretized analog of the modified
multipole moments for $0 \leq l \leq l_{\text{max}}$ and $1 \leq m
\leq l$, an operation that scales as $N^3$. We then directly compute
the value of the potential on all of the $6N^2$ boundary zones. Since
the multipole moments only need to be calculated once per Poisson
solve, the full operation scales only as $N^3$. The amount of time
required to calculate the boundary conditions is directly related
to the chosen value of $l_{\text{max}}$, so there is a trade-off
between computational expense and accuracy of the result.

As a demonstration of the method's accuracy, we consider the case of two 
white dwarfs of mass ratio 2/3, using the initialization procedure described below 
in \autoref{sec:software}. We terminated the simulation just after
initialization, so that we perform only an initial Poisson solve for this 
density distribution. We did this for values of $l_{\text{max}}$ ranging
from 0 to 20, and we also did this using the numerically exact solution 
provided by \autoref{eq:direct_sum}.  Defining the $L^2$
norm of a field $f$ as
\begin{equation}
  \| f \|_2 = \left(\sum_{i,j,k} \Delta x\, \Delta y\, \Delta z\, f_{ijk}^2\right)^{1/2},
\end{equation}
we computed the $L^2$ error of $\Phi$ on the entire domain for multipole 
boundary conditions, which we call $\Phi_l$, relative to $\Phi$ 
obtained using the exact boundary conditions:
\begin{equation}
  \text{Error}_l = \frac{\|\Phi_l - \Phi_{\text{exact}}\|_2}{\|\Phi_{\text{exact}}\|_2}.
\end{equation}
The result is shown in \autoref{fig:bc_comparison}. At $l_{\text{max}} = 6$,
the error is already well below $10^{-4}$ and we adopt this as our default 
choice for all simulations with Poisson gravity. In \autoref{sec:kepler} we 
show that there are no gains to be had by increasing the accuracy further. At 
very high orders ($l \ge 18$) the approximation breaks down, as seen in \autoref{fig:bc_comparison}. 
This is a result of the ambient material on the grid. At each boundary point we 
assume that all of the mass on the grid is contained within a sphere whose radius is 
the distance from that boundary point to the center of the domain. This does not hold 
for boundary points in the centers of domain faces, because of the material in the 
domain corners. This can be fixed by using multiple mass shells at diferent radii, but the error 
is negligible in practice for the values of $l_{\text{max}}$ that we use.
\begin{figure}[h!]
  \centering
  \includegraphics[scale=0.875]{plots/bc_comparison}
  \caption[Error in potential from multipole boundary conditions]
          {Error of $\Phi$ on the computational domain for a binary white dwarf simulation 
           whose boundary conditions were computed using various values of the maximum multipole order,
           relative to the exact solution determined by a brute force sum on the boundaries.
           Circles represent the error at integer values, and they have been connected by a smooth 
           line to guide the eye.\label{fig:bc_comparison}}
\end{figure}

\subsubsection{Convergence Testing}\label{sec:gravity_convergence_testing}

Since the results of a merger simulation depend strongly on gravity,
it is important to check whether proper numerical convergence is
achieved for the Poisson solver. To do so, we created a simple test
that initializes a sphere of radius $R$ and uniform mass density $\rho$
onto our grid, and used \castro\ to calculate the gravitational
potential $\Phi$ of this setup. We ensure that $R$ is an integer
multiple of the grid spacing, and the center of the sphere is at the
origin. The problem domain for our simulations is $[-1.6\ \text{cm}, 1.6\ \text{cm}]^3$, and
we take $R = 1.0\ \text{cm}$ and $\rho = 10^3\ \text{g cm}^{-3}$. 
The zones with $r > R$ are filled with an ambient material of very low density 
($10^{-8}\ \text{g cm}^{-3}$). We run this problem at multiple 
resolutions corresponding to jumps by a factor of two. For
comparison, at each grid point we evaluate the analytical potential of
a uniform sphere, which can be easily determined using Gauss' law:
\begin{equation}
  \Phi_{\text{sphere}}(r) = -\frac{GM}{r} \times \begin{cases} (3R^2 - r^2)/(2 r^2) & r \leq R \\ 1 & r > R \end{cases},\label{eq:sphere-analytical}
\end{equation}
where $M = 4\pi R^3 / 3$ is the mass of the sphere. We measure the 
numerical error by calculating the $L^2$ norm of the error and 
normalizing it by the $L^2$ norm of the analytical solution:
\begin{equation}
  \text{Error} = \frac{\|\Phi - \Phi_{\text{sphere}}\|_2}{\|\Phi_{\text{sphere}}\|_2}.
\end{equation}
We define the order of convergence $p$ between two simulations with a jump 
in resolution of integer factor $m > 1$ as
\begin{equation}
  p = \text{log}_{m}\left(\frac{\text{Error}_{\text{low}}}{\text{Error}_{\text{high}}}\right).
\end{equation}
Here $\text{Error}_{\text{low}}$ is the $L^2$ error at the lower resolution 
and $\text{Error}_{\text{high}}$ is the $L^2$ error at the higher resolution.
We expect the error to converge at $p = 2$ given the discretization we choose. 
For all simulations in this section and for all our main science simulations,
we choose a relative error tolerance of $10^{-10}$ to be satisfied in the multigrid solve.
The results of this test are plotted in \autoref{fig:gravity_convergence}. 

We find that at low resolution convergence is actually substantially better 
than second-order. The explanation for this is that we are attempting to 
model a spherical object on a rectangular grid. This results in two sources of error.
First, at very low resolution, the object does not look very spherical due to the rectangular 
grid representation, so the potential it produces is not quite that of a sphere. 
As the resolution is increased, the distribution of the mass on the grid will change.
\begin{figure}[h!]
  \centering
  \includegraphics[scale=0.85,trim=0.075in 0.0in 0.7in 0.0in,clip]{plots/phi_comparison}
  \caption[Analytical versus numerical solution of the potential]
          {Comparison of the \castro\ gravitational potential to the analytical solution for: 
           a sphere of uniform density; the same sphere, but with the potential normalized using the 
           actual amount of mass on the grid instead of the mass of a perfect sphere; and, a 
           cube of uniform density. Plotted also is a notional curve whose slope represents
           perfect second order convergence.\label{fig:gravity_convergence}}
\end{figure}
Second, the total amount of mass on the grid will change as the sphere fills out. 
So we are combining the true accuracy bonus from increased resolution 
with the artificial accuracy bonus from getting closer to solving the problem 
we are supposed to be solving. At high resolution this effect levels off, though, 
as the representation of the sphere is not significantly different in 
our two highest resolutions shown. For example, at $128$ zones per dimension 
the amount of mass on the grid happens to be slightly closer to the true spherical 
mass than at $256$ zones per dimension.
We can eliminate the second source of error by changing the density on  
the grid so that the total mass $M$ is actually what we intend it to be.
The resolution study for this case (the ``normalized sphere'') is also
plotted in \autoref{fig:gravity_convergence}. At low resolution we still obtain
convergence slightly better than second-order, indicating that we 
have not eliminated the geometrical problem of the mass distribution changing.

The only way to fully eliminate this effect is to use a test problem that
does not change with resolution. The obvious companion problem is a cube of
uniform density $\rho$, where now $R$ is half of the side length of
the cube. At each resolution we use the same $R$ as for the sphere,
which ensures that the cube always fills exactly the same fraction of
the domain and thus has the same mass, so the only improvement comes
from better sampling at higher resolution. The gravitational potential for this
object has been worked out analytically by \citet{waldvogel:1976} (see
also a similar result by \citet{hummer:1996}, and an earlier calculation 
by \citet{macmillan:1958}). The potential is given in
Equation 15 of that paper\footnote{The last term in that equation is missing a factor of
$1/2$, which destroys the symmetry. We have inserted this missing factor and
performed a simple coordinate transformation so that the center of
the cube is at the origin.}:
\begin{align}
  \Phi_{\text{cube}}(x,y,z) &= -G\rho\sum_{i,j,k=0}^1\left[x_i y_j\, \text{tanh}^{-1}\left(\frac{z_k}{r_{ijk}}\right)\right. \notag \\
  &+ \left. y_j z_k\, \text{tanh}^{-1}\left(\frac{x_i}{r_{ijk}}\right) + z_k x_i\, \text{tanh}^{-1}\left(\frac{y_j}{r_{ijk}}\right) \right.\notag \\
  &\left. - \frac{x_i^2}{2}\,\text{tan}^{-1}\left(\frac{y_j z_k}{x_i r_{ijk}}\right) - \frac{y_j^2}{2}\,\text{tan}^{-1}\left(\frac{z_k x_i}{y_j r_{ijk}}\right) \right. \notag \\
  &- \left. \frac{z_k^2}{2}\,\text{tan}^{-1}\left(\frac{x_i y_j}{z_k r_{ijk}}\right)\right]
\end{align}
where $x_0 = R + x$, $x_1 = R - x$, $y_0 = R + y$, 
$y_1 = R - y$, $z_0 = R + z$, $z_1 = R - z$, 
and $r_{ijk} = \sqrt{x_i^2 + y_j^2 + z_k^2}$. We note that if implemented in 
\texttt{Fortran} or \texttt{C}/\texttt{C++}, the inverse hyperbolic tangent used here is
\texttt{atanh} and the inverse tangent is \texttt{atan} (\textit{not}
\texttt{atan2}). This formula is valid both inside and outside the
cube. The normalized $L^2$ error for this problem is also shown
in \autoref{fig:gravity_convergence}, and only for this problem 
do we obtain perfect second-order scaling at all resolutions.

The main lesson here is that in a convergence study, it is important
to ensure that the physical problem does not change with
resolution. Since in the case of spherical objects on rectangular
grids the effect may be to artificially boost convergence with resolution,
in a simulation with spherical objects like stars one can envision a
scenario of being fooled into believing apparently good convergence
results that are simply a convolution of artificially high
gravitational convergence and poor convergence in the hydrodynamics. A
convergence study in this case is only fully valid if there is reason
to be confident that this effect is negligible compared to other
factors.

\subsection{Rotation}
\label{sec:rotation}

For the evolution of binary systems, it is most natural to evolve the
two stars in a frame that is co-rotating at the same period as the
orbital period. Since the publication of the original code paper, \castro\ 
now has the ability to evolve systems in a rotating reference frame. 
Source terms corresponding to the Coriolis and centrifugal 
force terms are added to the momentum and energy equations. In this frame, 
the stars essentially remain stationary in their original positions due to the
centrifugal force supporting against the gravitational attraction, and
will remain this way as long as significant mass transfer does not
occur. \cite{swc:2000} demonstrated (in the context of neutron star
mergers) that conservation of angular momentum is much easier to
obtain in the rotating reference frame than in an inertial frame in
which stars advect large amounts of material around the domain. We
wish to emphasize that although it is commonly stated in the
literature that fixed-mesh codes poorly conserve angular momentum,
it is only generally true that mesh-based codes do not exactly conserve 
angular momentum when the equations are written in conservative form
for linear momentum. Indeed, \cite{motl:2002} and \cite{byerly:2014} 
have evolved binary systems using the hydrodynamics equations written 
in a form that explicitly conserves angular momentum, and it is 
straightforward to convert an existing grid-based code to solve 
the system of equations that \citeauthor{byerly:2014} present.
Additionally, the extent to which angular momentum conservation is violated in our code
is a function of the resolution. When the resolution is sufficiently high, 
excellent conservation properties can result. At reasonable resolution 
for a binary orbit our code conserves angular momentum well enough 
to keep the stars stable for a large number of orbits; however, at moderate 
resolution in an inertial frame, there is a secular loss of angular 
momentum that eventually will result in a spurious merger.
We note that as the stars begin to coalesce, the rotating reference frame
will no longer provide a good approximation to the spatial motion of
the stars and then they will begin to significantly move around the
domain. This is not necessarily problematic because the most important
feature of the rotating frame is that it helps ensure that the initial
coalescence is not the result of spurious numerical loss of angular
momentum. When significant mass transfer sets in and evolution
proceeds on a dynamical timescale, the conservation properties may be
slightly worse but angular momentum conservation is also less
important.

In a rotating reference frame with angular frequency vector $\bm{\omega}$,
the non-inertial contribution to the momentum equation is:
\begin{equation}
  \left.\frac{\partial(\rho \mathbf{u})}{\partial t}\right|_{\text{rot}} = -2\, {\bm\omega} \times (\rho\mathbf{u}) - \rho {\bm\omega} \times \left({\bm\omega} \times \mathbf{r}\right).
\end{equation}
Here $\mathbf{r}$ is the position vector with respect to the origin. Typically we choose $\bm{\omega} = (0, 0, 2\pi / T)^T$,
with the rotation axis coincident with the $z$ axis at $x = y = 0$.
$T$ is the rotation period, which is the most natural quantity to specify
for a rotating stellar system. As described in Appendix A of \cite{wdmergerI}, we include this source term
in the edge state prediction in a way that is analogous to the gravity source.
We evaluate all quantities at cell centers. We use the same predictor-corrector 
approach that we use for the gravity source terms to the momentum equations. A slight 
difference is that the Coriolis force for each velocity component is coupled to other velocity 
components. If the rotation is about the $z$-axis, then the discrete update to 
$u^{n+1}$ depends on the value of $v^{n+1}$, and vice versa. If we fix the value of 
the time-level $n+1$ quantities after coming out of the hydrodynamics update, there 
would be a slight inconsistency between the $x$ and $y$ components of the velocity. 

We propose a more accurate coupling that directly solves this implicit system of coupled 
equations. We denote by $(\widetilde{\rho \mathbf{u}})$ the value of the momentum after 
updating it with the centrifugal force, and the time-level $n$ Coriolis force. The remaining 
update for the time-level $n+1$ Coriolis force then appears as:
\begin{equation}
  (\rho \mathbf{u})^{n+1} = (\widetilde{\rho\mathbf{u}}) + \frac{\Delta t}{2} \left(-2\, {\bm\omega} \times (\rho\mathbf{u})^{n+1}\right)
\end{equation}
To proceed further, we assume that the rotation is about the $z$ axis with frequency $\omega$. 
Then there is no update to the $z$-momentum, and the other equations are:
\begin{align}
  (\rho u)^{n+1} &= (\widetilde{\rho u}) + \omega \Delta t (\rho v)^{n+1} \\
  (\rho v)^{n+1} &= (\widetilde{\rho v}) - \omega \Delta t (\rho u)^{n+1}
\end{align} 
We can directly solve this coupled system:
\begin{align}
  (\rho u)^{n+1} &= \frac{ (\widetilde{\rho u}) + \omega \Delta t (\widetilde{\rho v})}{1 + \omega^2 \Delta t^2} \\
  (\rho v)^{n+1} &= \frac{ (\widetilde{\rho v}) - \omega \Delta t (\widetilde{\rho u})}{1 + \omega^2 \Delta t^2}
\end{align}
We use this form of the momentum update in \castro. This improvement is small
but increases the accuracy of our rotating white dwarf systems over long time-scales.

The update to the energy equation can be determined by taking the dot product of the velocity
with the momentum source terms. The Coriolis term vanishes identically, and so
the Coriolis term does no work on the fluid. The update from the centrifugal force becomes
\begin{equation}
  \left.\frac{\partial(\rho E)}{\partial t}\right|_{\text{rot}} = \frac{1}{\Delta V}\int \rho \mathbf{u} \cdot \mathbf{f}^R\, dV,
\end{equation}
with $\mathbf{f}^R \equiv  -{\bm\omega} \times \left({\bm\omega} \times \mathbf{r}\right)$. 
This expression is identical in form to the gravity source under the interchange of $\mathbf{g}$ with $\mathbf{f}^R$.
As observed by \cite{marcello:2012}, we can similarly write down a rotational potential,
\begin{equation}
  \Phi^R = \frac{1}{2} \left| {\bm\omega} \times \mathbf{r} \right|^2.
\end{equation}
In the presence of rotation the conserved total energy becomes:
\begin{equation}
  \int dV (\rho E_{\text{tot}}) = \int dV \left( \rho E + \frac{1}{2} \rho \Phi + \rho \Phi^R \right).
\end{equation}
Given that we can write down a potential energy for the rotation field, then we can use the machinery of 
\autoref{sec:gravity_hydro_coupling}. We again continue to evolve explicitly an equation for 
the gas energy, and allow it to change in response to work done by or on the rotational potential.
\begin{align}
  \left.\Delta(\rho E)\right|_{\text{rot}} &= -\frac{1}{2}\sum_{f} \Delta \rho_{f} (\Phi^R_{f+1/2} - \Phi^R_{f-1/2})
\end{align}

We apply the rotational forces after the gravitational forces, but 
there is some freedom in the order in which to apply the gravitational and rotational terms.
This order may matter because the Coriolis force depends on the fluid velocity, and 
in the predictor-corrector approach, we use the velocities both at 
time-level $n$ and time-level $n+1$. If we update the latter with the gravitational force, 
then the Coriolis force sees a different velocity than the one obtained through the 
pure hydrodynamics step. (The energy equation does not face the same issue in our new formulation,
because the velocities used are always the time-level $n+1/2$ values coming from the Riemann solver.)
In practice, this does not matter significantly for our simulations in this work 
because the centrifugal force plays the dominant role in maintaining stability of non-contact 
binary systems, and the centrifugal force does not depend on the fluid velocity.
This issue may be worth exploring in future work in situations where the Coriolis 
term is non-negligible in determining the system evolution.

In all simulations performed in a rotating reference frame, we transform all relevant
quantities back to the inertial reference frame when reporting them in analysis routines 
and visualization (though the data is saved to plotfiles while still in the rotating frame). In particular,
for every zone we adjust the position, momentum, and energy to account for rotation.
If the position is $\mathbf{x}$ in the inertial frame and $\mathbf{x}^\prime$ in 
the rotating frame, and the rotation vector is $\bm{\omega}$, the transformation rules are:
\begin{align}  
  \mathbf{x}(t) &= \mathbf{R}\mathbf{x}^\prime(t) \\
  \mathbf{v}(t) &= \mathbf{v}^\prime(t) + \bm{\omega} \times \left(\mathbf{R} \mathbf{x}^\prime(t)\right)
\end{align}
The rotation matrix $\mathbf{R}$ is:
\begin{align}
  \mathbf{R} &= \mathbf{R}_z({\bm{\theta}}_3) \mathbf{R}_y({\bm{\theta}}_2) \mathbf{R}_x({\bm{\theta}}_1)
\end{align}
where $\mathbf{R}_x$, $\mathbf{R}_y$, and $\mathbf{R}_z$ are the standard rotation matrices about 
the $x$, $y$, and $z$ axes, and $\bm{\theta} = \bm{\omega} t$.



\subsection{Nuclear Network}
\label{sec:network}

White dwarfs are mainly composed of $\alpha$-chain particles, primarily ${}^4$He,
${}^{12}$C, ${}^{16}$O, ${}^{20}$Ne, and ${}^{24}$ Mg. Therefore the core of
any network appropriate for modeling nuclear burning in white dwarfs will be
these alpha chain nuclides, with the idea being that links up the $\alpha$-chain
will eventually get us to ${}^{56}$Ni, the nuclide responsible for the
energy output of Type Ia supernovae. In this paper we consider four networks
to do this, presented in order of increasing complexity. The most simple is
\isoseven\ \citep{timmes:2000}, which includes all of the aforementioned isotopes and
${}^{28}$Si (see also \citet{hix:1998}). ${}^{28}$Si effectively measures the
equilibrium state of silicon-group elements, and ${}^{56}$Ni effectively measures
the equilibrium state of iron-group elements, with the link between them governed
by the effective loss or gain of seven $\alpha$-particles. This type of network
was used by \citet{rosswog:2009} for their collision calculations in SPH.

Next is \aproxthirteen\ \citep{timmes:1999,timmes:2000}. This includes
all of the isotopes of \isoseven, and all of the $\alpha$-chain particles between
silicon and nickel (${}^{32}$S, ${}^{36}$Ar, ${}^{40}$Ca, ${}^{44}$Ti, ${}^{48}$Cr,
and ${}^{52}$Fe). This network was used by \citet{hawley:2012} and \citet{raskin:2010}.
\citet{loren-aguilar:2010} and \citet{garcia-senz:2013} used a very similar network
that included additionally ${}^{60}$Zn. The \aproxnineteen\ network \citep{timmes:1999}
builds on \aproxthirteen\ by including isotopes for hydrogen burning and explicit
tracking of photodisintegration into ${}^{54}$Fe. This network was used by
\citet{kushnir:2013}, \citet{kushnir:2014}, \citet{rosswog:2009} for their
calculations with FLASH, and \citet{papish:2015}. Finally we will also
consider \aproxtwentyone, which includes all of the above plus ${}^{56}$Cr
and ${}^{56}$Fe and related reaction links. The primary virtue of using
the latter two networks is that they allow us to track changes away from
$Y_e = 0.5$.

All four of these networks have been ported into a form that is consistent
with the \boxlib\ codes, in the freely available \microphysics\ code
repository\footnote{\microphysics\ can be obtained at \url{https://github.com/BoxLib-Codes/Microphysics}.},
a collection of microphysical routines that are designed to be used in our
hydrodynamics codes. These can be easily swapped at compile time by using the 
appropriate makefile variable.



\subsection{Nuclear Burning}
\label{sec:burning}

Given a set of nuclides and the reaction links between them, we now consider
how a burning step is performed in our software. The goal is to integrate the
vector ${\bm{Y}} = (Y_1, Y_2, \ldots, Y_n, e, T)$, where $Y_{n} = X_{n} / A_{n}$
is the molar fraction of species $n$, with $X_n$ the mass fraction and $A_n$ the
mass number of that species, $e$ is the energy released during the burn, and
$T$ is the temperature. The equation describing its evolution is given by
\begin{equation}
  \frac{d\bm{Y}}{dt} = f(\mathbf{Y}),
\end{equation}
where the components of the right-hand-side for the species come from the particular
nuclear burning network we are using. The energy release $e$ of the zone's burn
will change when the nuclear abundances evolve, according to
\begin{equation}
  \frac{\partial e}{\partial t} = N_A \sum_{n} \frac{\partial Y_{n}}{\partial t} m_{n} c^2,
\end{equation}
where $c$ is the speed of light and $m_n$ is the mass of each nuclide.

In a hydrostatic burn, we keep $\rho$ and $T$ fixed throughout, and use
the energy released at the end to compute a final temperature that is
thermodynamically consistent with the new internal energy. By contrast,
in a self-heating burn, we allow the temperature to evolve in response
to the burning (see\footnote{In the cited paper, a term based on the
thermodynamic chemical potential was included; we now believe
that it is incorrect to include such a term in the burn, since it
automatically sums to zero analytically.} \citet{maestro3}):
\begin{equation}
  \frac{dT}{dt} = \frac{1}{c_V}\frac{\partial e}{\partial t}
\end{equation}
(Although $T$ evolves during the burn so that the integration is physically
accurate, as for the hydrostatic method we discard the final value
for $T$ at the end of the burn and recompute a temperature for the zone that is
consistent with its new internal energy.) Here $c_V$ is the specific heat at
constant volume, which is provided by the equation of state.  During this burn,
we can keep $c_V$ constant using its initial value, or at each step we
can choose to re-evaluate the equation of state using the latest value of $(\rho, T)$.
The latter is more expensive but also more accurate, and we use it in this paper.
In practice we find that the cost is negligible in comparison to the more expensive
parts of the calculation, and it can significantly speed up convergence near NSE.
A third option presented by \citet{raskin:2010} is a so-called ``hybrid'' mode.
In this mode, by default we do a hydrostatic burn. If that burn fails, or if the net
energy change is negative, we do the burn again in self-heating mode. A final option
is a burn that limits the changes due to a burn to avoid numerically unstable burning.
This mode is discussed in \autoref{sec:unstable_burning} and we will call it a
``suppressed'' burn for the remainder of this work. All four options
are implemented in our burner software. The simulations shown in this work all
use the self-heating mode unless otherwise specified.

In our \microphysics\ repository we provide several software options for
solving a set of coupled stiff ODEs. For this work we use an implementation of
the well known variable-order Richardson extrapolation method presented by Stoer and
Bulirsch \cite{stoer:1980}, that is similar to the integrator which ships with
the original versions of the networks mentioned above. Previous work using the \boxlib\
codes typically used the classic \vode\ integrator \cite{vode}. We do maintain a version
of the software that is compatible with our software interfaces in the \microphysics\
repository, but we have largely shifted to integrators which are written in modern
Fortran as a consequence of our efforts to run our codes on GPUs. The Stoer and Bulirsch
integrator we provide satisfies this criterion; we also provide a rewrite of the VODE
BDF algorithm that uses modern Fortran features. The Stoer and Bulirsch algorithm
relies on a uniform relative error tolerance for all of the ODEs in the system, which
we set at $10^{-6}$ for all simulations described here.

\subsubsection{Coupling to Hydrodynamics}
\label{sec:burnerhydrocoupling}

In \castro, the reactions are coupled to the hydrodynamics using Strang splitting.
In a given timestep advance $\Delta t$, we first evolve the reactions alone through
a time interval $\Delta t / 2$. Then, we evolve the hydrodynamics for $\Delta t$,
and we evolve the reactions again for a further $\Delta t / 2$. The principal
drawback of this approach is that the reactions and the hydrodynamics can become
decoupled from each other. A common solution to this problem presented in
the literature has been to limit the size of the timestep and thereby limit the
extent of this decoupling \citep{raskin:2010,hawley:2012}, which we adopt here 
and have implemented in \castro. Defining the nuclear energy injection timescale 
$\tau_e$, and the species evolution timescale $\tau_{X_k}$,
\begin{align}
  \tau_e &\equiv \frac{e}{|\dot{e}|} \\
  \tau_{X_k} &\equiv \frac{X_k}{|\dot{X_k}|},
\end{align}
where $\dot{e}$ is an estimate of the time rate of change of the internal energy
from nuclear burning, and $\dot{X_k}$ is an estimate of the time rate of change 
of the mass fraction of the species with index $k$, we define burning-limited 
timesteps $\Delta t_{be}$ and $\Delta t_{bX_k}$:
\begin{align}
  \Delta t_{be} &= f_{e}\, \tau_e \label{eq:timestep_e}\\
  \Delta t_{bX_k} &= f_{X}\, \tau_{X_k}. \label{eq:timestep_X}
\end{align}
Given an estimate for $\dot{e}$, the factor $f_{e}$ determines by what 
fraction we would like to allow the internal energy to change
in the current timestep, under the assumption that $\dot{e}$ does not change from
timestep to timestep. Similarly, given an estimate for $\dot{X_k}$, the factor $f_{X}$ 
determines the maximum change in the mass fraction of any species. By making 
$f_{e}$ and $f_{X}$ smaller, we can control the magnitude of the decoupling 
between the reactions and the hydro. We choose by default $f_{X} = f_{e} = 0.1$, 
a choice which is slightly smaller than others in the literature. (Note that previous 
collision papers we are aware of only limited based on changes in internal energy, 
not on cahnges in species.)  The sensitivity of results to this choice will be discussed in 
\autoref{sec:collision_parameters:timestepping}. The factors $f_{e}$ and $f_{X}$ can be set at runtime in \castro.

At the start of each advance, we limit the size of the timestep to be the smaller
of the minimum hydro timestep (limited by the CFL condition), and the minimum of all the
burning timesteps across all zones. To do this, we need a method for determining 
$\dot{e}$ and $\dot{X_k}$. A typical choice in the literature has been to set, for example,
\begin{equation}
  \dot{e} = \frac{e^{n} - e^{n-1}}{\Delta t^{n-1}}, \label{eq:burning_limiter_mode_4}
\end{equation}
where is $e^n$ is the internal energy at the start of the current timestep and
$e^{n-1}$ is the internal energy at the start of the previous timestep. 
The obvious analogue is used for constructing the species rate of change.
However, there are alternative methods of constructing this derivative estimate, 
and we have found that these different methods have measurable consequences.
We define four separate methods for calculating the time derivative, with 
the above being mode 4. Mode 3 is similar to mode 4 but replaces the
denominator in \autoref{eq:burning_limiter_mode_4} with the change in 
internal energy over the last timestep only from the nuclear reactions.
Mode 2 is the same as mode 3 but we only use the change in internal 
energy from the most recent nuclear burning step, that is, the second-half
of the Strang-split burning from the last timestep (the denominator 
then becomes $\Delta t / 2$). In mode 1, the most accurate option and 
the current default in \castro, we evaluate the right-hand-side of the 
burning network given the current state to explicitly obtain the 
instantaneous value of $\dot{e}$ and $\dot{X_k}$. 

To understand the consequences of this choice, and more broadly to 
understand the limitations of Strang splitting, we consider the 
basic outline of a single-level advance in an advection-reaction system:
\begin{enumerate}
  \item Evaluate timestep $\Delta t$ for the current advance
  \item Advance the nuclear burning network by $\Delta t / 2$
  \item Advance the hydrodynamics by $\Delta t$
  \item Advance the nuclear burning network by $\Delta t / 2$
  \item Return to Step 1
\end{enumerate}
Now, consider that during a head-on collision, initial nuclear burning 
will occur at the contact point between the two stars. Because of 
the staggered updates from splitting, the evolution effectively progresses 
as a cycle between burning for $\Delta t$ and getting fresh material 
advected into the contact point by the hydro update for $\Delta t$. 
When the collision begins, $\Delta t$ is controlled by the hydrodynamic 
stability criterion, and may be large enough that it is possible for 
the burning advance in Step 4 to completely burn the freshly advected 
material all the way to NSE. Consequently the evolution is no longer 
a good approximation to smooth burning of the in-falling material but
rather separate discrete burning and hydro steps, and the nature of 
the burning evolution will be quite different. Furthermore, by the 
time we return to Step 1 and estimate the next timestep size, all 
of the burning rates will be small again, and the instantaneous 
timestep limiter of mode 1 may actually substantially overestimate 
the needed timestep. The other modes will see that the energy/species  
substantially changed over the last timestep, but will still 
overestimate the needed timestep because the burning was quiescent
for at least some portion of the last advance. In particular, our 
experience has shown that using only the energy change criterion
is particularly susceptible to this phenomenon; silicon-group 
material can build up without changing the internal energy by a 
large fraction, so the timestep limiter is never triggered, and 
then in a single step a substantial amount of iron-group elements 
can be be generated. This can have non-trivial effects on the 
total amount of iron-group elements generated over the course of
the simulation. We have found that the addition of the 
limiter based on changes in species functionally precludes this.

With the timestep limited the way we advocate in this paper, 
the timesteps are generally short enough so that the errors 
due to splitting are small. Other approaches to the coupling 
between reactions and hydrodynamics have been proposed in the 
broader literature, especially iterative methods such as 
deferred corrections that allow each of these operators to 
feel the lagged effects of the other operators. For example,
in the context of low Mach number flows, \cite{nonaka:2012} have
used the method of spectral deferred corrections \citep{SDC} to
couple their advection-diffusion-reaction equation set. In our
context this would involve using the advection as a source term
for the burning network, and symmetrically including a source
term to the hydrodynamics corresponding to the energy release
from the burning. We are presently investigating such a method,
and it may form the basis of further work on this subject.

Now we return to a point we hinted at above. The timestep
will only actually satisfy the energy criterion
$\Delta t \leq f_e \tau_e$ and species criterion
$\Delta t \leq f_{X_k} \tau_{X_k}$ when the estimates for
$\dot{e}$ and $\dot{X_k}$ we generate are at least as large
as the actual rate of change of energy and mass fractions
over the timestep. However, this can assumption can fail
during periods of runaway burning when the rate of change
of these quantities is highly nonlinear. We may not want
to neglect the errors caused by this approximation
because they may build up over an extended period of nonlinear
evolution and perhaps substantially change the final results.
To this end, we have implemented a timestep retry option in
\castro, which re-computes an advance if it violated the
stability criteria as judged from the end of the timestep.
However we have found that the benefits for this problem are
small and thus we do not use it for the simulations here.

One other point worth noting for the coupling of the reactions
and the hydrodynamics relates to the dual-energy formalism used
by \castro\ and many other hydrodynamics codes \cite{bryan:1995}.
\castro\ evolves separately equations for the total energy $(\rho E)$ and
for the internal energy $(\rho e)$. The former is conservative while the latter
is not, so for accurate hydrodynamics we prefer to use the total
energy when possible and to use the evolved internal energy variable
only in situations where the kinetic energy dominates the contribution
to the total energy. Indeed, for the cosmological purpose for which
this was originally developed, \cite{ENZO} choose a value $\eta_2 = 0.1$
so that $e$ is only updated to be equal to $E - \mathbf{v}^2/2$ if
$e > \eta_2\, E$. In other cases the evolved value of the internal
energy variable is preserved. However, this choice does not work
for our application, because the ultimate cause of the nuclear
burning in a white dwarf collision is the rapid conversion of
kinetic energy from the white dwarf bulk motion to thermal energy
as the white dwarfs slam into each other. Keeping $\eta_2$ large
prevents this from happening, and consequently the temperature
will never reach values high enough to generate significant
amounts of nickel. For this paper we choose $\eta_2 = 10^{-4}$,
which is low enough for the correct conversion of kinetic energy
but not so low that we need to be concerned about roundoff issues
caused by subtracting kinetic from total energy. This is also
the value that was used by \cite{hawley:2012}.

\subsubsection{Numerically Unstable Burning}
\label{sec:unstable_burning}

\citet{kushnir:2013} point out that an inappropriate timestep is 
not the only way for the numerical discretization to cause 
severe errors in the burning. Another failure mode is when
the energy injection timescale
$t_e$ is shorter than the sound-crossing time $t_s$ in a zone.
When the sound-crossing time is too long, energy is built up in
a zone faster than it can be advected away by pressure waves.
This is obviously a problem inherent to numerically discretized
systems as the underlying fluid equations are continuous.
This can lead to a numerically seeded detonation caused by the
temperature building up too quickly in the zone; the detonation
is spurious in this case and should be avoided if possible.
The goal is to ensure that the following condition holds:
\begin{equation}
  t_s \leq f_{s} t_e \label{eq:burning_limiter_2}
\end{equation}
The sound crossing time, $t_s$, is given by $\Delta x / c_s$, 
where $c_s$ is the sound speed and $\Delta x$ is the (minimum) 
zone width. The parameter $f_{s}$ then determines the minimum
ratio of the nuclear energy generation timescale to the 
sound-crossing time. \citet{kushnir:2013} choose $f_{s} = 0.1$ 
for their simulations, and we do too (this parameter can be set 
at runtime in \castro).

\citet{kushnir:2013} implemented this criterion by artificially 
limiting the magnitude of the energy release after a burn. We
too have developed an option for our burner to do this,
the ``suppressed'' burning mode. In a suppressed burn, we limit
the changes to the state so that \autoref{eq:burning_limiter_2}
is always satisfied. To achieve this we directly multiply the
right-hand-side vector in the integration by a constant factor $F$
for all variables, where $F$ is the multiplicative factor needed to
be applied to $\dot{e}$ such that the equality in \autoref{eq:burning_limiter_2}
holds. (If the inequality is already satisfied, then the integration
vector is not modified.) We fix $t_s$ to be the value of the sound
crossing time at the beginning of the burn (that is, we do not
update it as the sound speed changes) and we fix the energy $e$
that goes into the estimate for $t_e$ to be the value of the
internal energy of the zone at the beginning of the burn. If
instead one allowed $c_s$ and $e$ to evolve with the burn, one
would obtain a less conservative limiter in the case of explosive
burning, as $c_s$ and $e$ are both increasing in this case.
As the point of the limiter is to ensure that the changes to the
\textit{original} energy are small enough so that the following
hydrodynamics update can advect away newly generated energy
quickly enough to avoid a nuerically seeded detonation,
we desire the most conservative version of the limiter. We discuss
our results with the suppressed burning mode in \autoref{sec:collision_parameters:burningmode}.
Note that we have found that with this option enabled, it typically takes
many more timesteps to complete a burn than in self-heating mode.

Regardless of whether the suppressed burning mode works for this
particular problem, it is not physical, so in general we have
taken a different approach. If we insist that we cannot
cannot directly control the energy injection timescale, we 
must find a way to alter the sound-crossing timescale. 
We can achieve this by adding levels of refinement in 
regions that do not satisfy \autoref{eq:burning_limiter_2},
which effectively lowers the $\Delta x$ and thus the
sound-crossing time. We keep tagging zones for refinement
based on this criterion until the criterion is satisfied
on the finest level. Since the concern is regions that 
may detonate, we also tag nearby zones in a buffer region
which do not themselves satisfy the criterion,
so that a detonation in a single timestep cannot 
escape into non-refined regions. The width of the buffer 
region should thus be at least as large as the number of 
timesteps before a regridding procedure is performed.
We choose a value of two for both the number of zones in the 
buffer region and the number of steps in between regrids,
for all simulations in this paper.

While this approach may add several AMR levels 
to a simulation, we agree with \citet{kushnir:2013} that 
solving this numerical instability is crucial to avoiding
early, unphysical detonations, and so it is unavoidable
if a correct evaluation of the burning phase is desired.
A simulation that does not solve this problem in this way
(or some analogous manner) will not obtain the correct amount 
of burning, and will not converge properly with resolution. 
Fortunately, the regions where this criterion is not satisfied 
are fairly localized on the grid, so the amount of additional 
computational work is not insurmountable for a single simulation.
However, this level of accuracy may make a parameter study of
the problem challenging given current computational resources.
When resources are limited, one can still use this method and set a
limit on the number of levels of refinement possible in the simulation,
so that the refinement will still go in the right places but may not
refine all of the way to the regime of guaranteed stability. 



\newpage
\section{Simulation Software}
\label{sec:software}

\subsection{White Dwarf Models}
\label{sec:initial_models}

\subsection{Initial State}
\label{sec:initial_state}

\subsection{Analysis}
\label{sec:analysis}

\subsubsection{Gravitational Waves}
\label{sec:grvatational_waves}



\newpage
\section{Verification Tests}
\label{sec:verification}

White dwarf merger simulations face a number of numerical difficulties,
including the typical issues that make any numerical hydrodynamics simulation
challenging, but also a number of difficulties that are
not present in single-degenerate Type Ia and core-collapse supernova
simulations.  Thus while the behavior of \castro\ for many standard hydrodynamics
test problems was detailed in the original code paper \citep{castro}, and the code
is regularly subjected to a battery of test problems that ensure it gives reasonable
results, the usual suite of problems needs to be complemented by a set of tests
that exercise the features unique to binary star systems. In \autoref{sec:gravity},
we gave examples of how the gravity solver can affect such a system, for example
through the boundary conditions on the potential and the way the gravitational source
term is added to the hydrodynamics update. There are also hydrodynamical issues that
are specific to the case where large amounts of material move at significant speeds
across the grid, and the merger process is just such a case. This bulk motion presents
an opportunity for advection errors to build up, and is only partially mitigated by
evolving the white dwarfs in a co-rotating frame. It is therefore important to be
aware of the behavior of the code in such circumstances.

So our focus here is on a subset of problems that highlight the special difficulties
introduced in merger simulations. These problems couple the hydrodynamics, gravity and
equation of state modules. We observe that while in most non-trivial
three-dimensional problems this creates a complexity that makes it
impossible to determine exact analytical solutions, it is
straightforward to devise problems for which certain global properties
should obey simple, expected behaviors. Where possible, these should
be quantified and a convergence study performed; where not, we should at least
be able to run the test and see whether the results make sense given what
we know about the properties of the system. We should also at least be able
to see whether the results converge with numerical resolution (even if we cannot
see whether this convergence is to the correct answer for the problem).
This is the focus of the current section.

\subsection{Maintaining Hydrostatic Equilibrium}
\label{sec:HSE}

In \autoref{sec:initial_models} we describe the process by which
we generate initial stellar models. While the 1D models are in
hydrostatic equilibrium to within a small error, interpolation onto
the 3D Cartesian grid will introduce perturbations into the solution
\citep{zingale:2002}. Although we ensure that the initial models are
generated with the same equation of state and are at least as well resolved as
our finest grid, there is still be a hydrodynamical error associated
with the fact that the rectangular grid cannot faithfully represent a
spherical star. Additionally, the gravitational potential obtained by
the multigrid solver will differ slightly from the one assumed by the
initial model, and the operator splitting between the gravity and
hydrodynamics should also result in small errors. As a result, we
expect that the star will oscillate slightly about an equilibrium
point, but that the amplitude of this oscillation should decrease with
increasing resolution.

\begin{figure}[h!]
  \centering
  \includegraphics[scale=0.8,trim=0.15in 0.0in 0.55in 0.5in,clip]{plots/single_star_static_1e3_radius}
  \caption[Effective radius evolution of a static white dwarf]
          {Time evolution of the effective radius of a $0.9 \msolar$ 
           white dwarf, seeded onto the grid using a one-dimensional hydrostatic
           model and evolved without further relaxation. The lines represent 
           different number of zones per spatial dimension; when this number is 
           greater than 256, it represents an effective resolution obtained 
           using AMR levels that cover the star. The radius is determined 
           using the volume of the grid that has a density greater than $10^3\ \text{g cm}^{-3}.$
           \label{fig:single_star_static_radius}}
\end{figure}

This problem was studied in the first \castro\ paper, but is worth
revisiting here. A single star explosion simulation may only last a
couple of seconds, and the \castro\ paper studied the behavior of the
star after one second of evolution. However, the dynamical timescale
of a typical carbon-oxygen white dwarf is on the order of 1--10
seconds. Additionally, a binary orbit is typically on the order of
10--100 seconds when a merger simulation starts, and with equilibrium
initial conditions the system may survive for tens of orbits before
the secondary is disrupted. When this does happen, we want to be
confident that it was because of the dynamics of the merger process
and not because of an instability in an individual star. Our goal here
is thus to install a single star onto our three-dimensional
coordinate grid and evolve it for a period of time long enough to
assess whether the star is truly stable, and to probe how the size of
deviation from equilibrium is affected by grid resolution.

We loaded a single star of mass $0.9\ \msolar$ onto the grid at the
origin, and evolved it for 200 seconds. Our diagnostic of choice is
the effective radius of the star, determined by the volume of the grid
that has a density greater than $10^3\ \text{g cm}^{-3}$ (see
\autoref{sec:software} for details on this measure). This choice
of density is intended to mark a reasonable outer edge to the star
that is not immediately susceptible to the numerical errors prevalent
near the physical edge of the star.
\autoref{fig:single_star_static_radius} shows our results at various
resolutions.  As expected, the star quickly approaches an equilibrium
size that is different (and in this case larger) than the
one-dimensional model, though the magnitude of this change becomes
smaller with resolution. The star is only approximately in equilibrium 
by this measure when the coarse grid of $256^3$ zones has a level of 
refinement that jumps by a factor of four. Even then there is a slight uptick
in the size toward the end, implying that the numerical stability is
not guaranteed for arbitrarily long timescales. For another view, we 
consider the kinetic energy on the grid, in \autoref{fig:single_star_static_ke}. 
This is a more holistic measure that weights the contribution by the density. 
At the end of the simulation the kinetic energy is not lower at the highest 
resolution than at the lower resolutions. This result suggests
that when constructing the equilibrium initial models that will form the
basis of later calculations, we should carefully monitor the evolution
of the stars when applying any artificial damping to cause the
merger, to ensure that the merger is due to this applied force and not
the intrinsic numerical instability of the stars.
\begin{figure}[h!]
  \centering
  \includegraphics[scale=0.80,trim=0.1in 0.0in 0.55in 0.45in,clip]{plots/single_star_static_ke}
  \caption[Kinetic energy evolution of static white dwarf]
          {Time evolution of the kinetic energy of a $0.9\, \msolar$ 
           white dwarf. The lines have the same meaning as in \autoref{fig:single_star_static_radius}.
           \label{fig:single_star_static_ke}}
\end{figure}

\subsection{Gravitational Free Fall}
\label{sec:Gravitational Free Fall}

A simple dynamical test to verify the coupling between the gravity and hydrodynamics in \castro\ is
the case of gravitational free fall. We place two stars on the grid 
in the manner of \autoref{sec:software}. The distance $a$ between 
them corresponds to a chosen orbital period $T$, consistent with the total
system mass $M$, but we disable the rotational source terms so that 
the stars start at rest in an inertial reference frame. 
Thus the stars will simply begin moving toward each other.
As long as the stars remain approximately spherical, the stars can be 
treated as point masses (this approximation only seriously breaks down after the stars
have come into contact). In dimensionless units where $r \to r / a$ and 
$t \to 2\sqrt{2}\pi t / T$, the simple free fall equation of motion governing the
distance $r$ between their centers of mass takes the form:
\begin{equation}
  \ddot{r}(t) = - \frac{1}{2r^2}.
\end{equation}
It is possible to derive a closed-form solution for the evolution time
as a function of separation by starting with the integral formulation,
\begin{equation}
  t(r) = \int_{1}^{r} \frac{dr}{v(r)}.
\end{equation}
The velocity $v$ (in dimensionless units) can be found by noting that 
$\ddot{r} = v\, dv / dr$ and then separating and integrating the equation 
of motion. This yields 
\begin{equation}
  v(r) = \sqrt{\left(\frac{1}{r} - 1\right)}.
\end{equation}
For our problem $0 < r \leq 1$, so this is always valid. Integrating, we find
\begin{equation}
  t(r) = \text{arccos}\left(\sqrt{r}\right) + \sqrt{r \left(1 - r\right)}. \label{analyticalFreeFall}
\end{equation}
so that the point of contact would occur at $t = 1$. We actually stop the simulation
at $t = 0.9$, which is when the effects from the extended sizes of the stars
starts to become important. The results of our simulation for our default $256^3$ zone 
uniform grid are shown in \autoref{fig:freefall}. They show excellent agreement
between the analytical solution and the simulation results.

\begin{figure}[h!]
  \centering
  \includegraphics[scale=0.80,trim=0.3in 0.0in 0.8in 0.5in,clip]{plots/freefall}
  \caption[Gravitational free-fall test]
          {Time evolution of two initially stationary white dwarfs,
           mutually attracted to each other by the gravitational force. The
           horizontal axis gives the separation of the white dwarfs, scaled
           to the initial separation, and the vertical axis gives the elapsed
           time of the simulation, scaled to the time it would take two point masses
           to collide. The solid curve shows the analytical result,
           calculated from Newtonian mechanics, and the circles show the
           samples from the time evolution with \castro. For visual clarity, we 
           show only a small fraction of the timesteps.\label{fig:freefall}}
\end{figure}

\subsection{Galilean Invariance}
\label{sec:galileo}

It is often stated in the literature that Eulerian methods for
hydrodynamics with grids fixed in space do not obey the Galilean
invariance of the underlying Euler equations, so that simulations
moving at a uniform bulk velocity appear different than an
equivalent stationary simulation (e.g. \cite{arepo}). If true, we need to understand 
the importance of this effect when deciding whether to trust the 
output of a code like \castro\ when applied for merger problems.
Recently, concern for the issue of Galilean invariance has come up in two ways which are of note for us 
in the present study. We explain these situations and display 
the results of tests we have run to determine whether this 
actually is a significant concern for our study.

\citet{arepo} (hereafter, S10) performed a Kelvin-Helmholtz instability test and showed
that (at low resolution) a fixed-grid code failed to develop the
expected fluid instability when the whole fluid was moving at a
strongly supersonic uniform velocity. (See also \citet{wadsley:2008}, 
who used the FLASH code to simulate a hot bubble subject to mixing 
by the Kelvin-Helmholtz instability, and also found that the mixing was affected by a 
uniform bulk velocity.) This contrasted with the results
of the moving-mesh code AREPO being presented in that study, which
demonstrated Galilean invariance even at large bulk velocities. 
Inability to correctly model the Kelvin-Helmholtz instability would 
have important consequences for how much we can trust the ability of 
\castro\ to test the violent merger progenitor model, where a detonation 
arises in the low-density material at the stellar surface. Shearing between 
the material flowing out of the secondary and material near the 
surface of the primary may trigger fluid
instabilities that play an important role in the evolution of that
gas, which is the site of the initial detonation in the prompt
explosion model. \citet{guillochon:2010} showed for their simulation
that Kelvin-Helmholtz instabilities produced this way may raise the
temperature of the accreting material enough to ignite a
detonation. Therefore if we are not correctly reproducing the
characteristics of the Kelvin-Helmholtz instability in the case where
there is significant mass motion on the grid, we cannot be confident
that a detonation (or lack thereof) is not numerically
seeded. 

\citet{robertson:2010} (hereafter, R10) observe that violation of Galilean
invariance of simulation results for the Euler equations occurs 
because of truncation error in the discretization of the fluid
equations. This takes the form of a numerical diffusion term which is
dependent on velocity (and also resolution). The advantage of a
moving-mesh code is that the mesh everywhere moves with the local flow
velocity, which substantially reduces the numerical
diffusion. R10 argue that the differences seen
between the moving-mesh and fixed-grid code are caused by the
interaction of this numerical diffusion with small-scale instabilities
(that may be physical or numerical) which couple with and
fundamentally alter the large-scale modes. Small-scale instabilities
are seeded by the choice of a sharp initial discontinuity between the 
fluids in the problem posed by S10. Crucially though,
R10 point out that this problem does not
converge with resolution (because the initial perturbation is too sharp 
and seeds numerical noise at the grid resolution level) 
and so it is not possible to know the correct
behavior of this problem. As such, we do not know whether the
small-scale modes found in AREPO are real, and the problem is not
useful in formally discriminating between methodologies. They instead
propose an alternate test with a smoother initial contact. This
converges to the same solution qualitatively in both the stationary
and bulk velocity cases, indicating that the code does generally
maintain Galilean invariance (to some specified error that depends on
resolution and the uniform flow speed).  We will see whether we can
reproduce this result.

A related question is whether our code reliably simulates the bulk
motion of the stars across the grid, and whether such bulk motion
affects the stability of the star. This concern is prompted by the
study of \cite{tasker:2008}, who studied the effect of uniform
translation on the stability of a spherically symmetric model for a
galaxy cluster. They compared the radial profile of the cluster at
initialization and after a period of time evolution. Using FLASH and
ENZO, they found that a static cluster retains its shape at high
enough resolution, while uniform translation of the cluster causes
mixing of the core material due to numerical diffusion which results
in an underestimation of the core's true density. The SPH codes they
used did a better job maintaining the core density. We will perform a
variant of this test using white dwarf models.

\subsubsection{Kelvin-Helmholtz Instability}
\label{sec:khi}

Following \cite{robertson:2010}, we set up a Kelvin-Helmholtz test in
the following way. The problem domain runs from 0 to 1 in both the $x$
and $y$ directions. This is a two-dimensional test, so we run
\castro\ in 2D mainly to avoid extra computational expense; in 3D, it 
would merely involve replicating the problem in the $z$ direction.
The problem involves a fluid slab of density $\rho_2 = 2.0$ traveling rightward in the
$x$-direction at velocity $v_2 = 0.5$, sandwiched by a fluid of
density $\rho_1 = 1.0$ traveling leftward at velocity $v_1 =
-0.5$. The density gradient is in the $y$ direction, so this creates a
velocity shear along the interface between the fluids. The density and
velocity distribution on the computational domain are given by:

\begin{figure}[h!]
  % Note that since we have periods in the filename, we need to double-
  % bracket the filename so that includegraphics doesn't think it's 
  % looking for something with the wrong extension. See:
  % http://tex.stackexchange.com/a/10575
  \centering
  \includegraphics[trim=0.75in 0.45in 0.85in 0.5in, clip, scale = 0.75]{{{plots/kh_t2.0_p2_low_res_collage}}}
  \caption[Kelvin-Helmholtz instability and bulk velocity (Robertson et al.)]
          {2D Kelvin-Helmholtz instability test at $t = 2.0$ for the initial 
           conditions given by \autoref{eq:kh_ic_b_ramp} and \autoref{eq:kh_ic_b}. 
           The rows each represent a different bulk fluid velocity $v$ and 
           the columns each represent a grid resolution $n$ (the number of 
           zones per spatial dimension). The highest velocity simulation, 
           $v=100$, corresponds to approximately Mach 70. Compare to 
           Robertson et al. (2010), Figure 7. \label{fig:kh_p2}}
\end{figure}

\begin{align}
  \rho &= \rho_1 + R(y)\left[\rho_2 - \rho_1\right] \\
  v_x  &= v_1 + R(y)\left[v_2 - v_1\right] \\
  v_y  &= v_{\text{bulk}} + v^\prime
\end{align}

Here $R(y)$ is a ramp function that describes the transition between
the two fluids, while $v_{\text{bulk}}$ is the bulk motion of the
fluid in the $y$ direction and $v^\prime$ is the velocity perturbation
that seeds the instability. The problem is established for two
sets of initial conditions (ICs), which we follow
R10 in calling ICs A and B. They differ in
their ramp function ($R_A$ and $R_B$ respectively), as well as the
initial perturbation ($v^\prime_A$ and $v^\prime_B$ respectively), and
the frequency of the perturbation ($n_A = 4$ and $n_B = 2$):
\begin{align}
  R_A &= \begin{cases} 0 & |y - 0.5| > 0.25 \\ 1 & |y - 0.5| < 0.25 \end{cases} \label{eq:kh_ic_a_ramp}\\
  R_B &= \Big\{\left[1 + e^{-2(y-0.25)/\Delta y}\right]\left[1 + e^{2(y-0.75)/\Delta y}\right]\Big\}^{-1} \label{eq:kh_ic_b_ramp}
\end{align}
\begin{align}
  v^\prime_A &= w_0\, \text{sin}\left(n_A\, \pi\, x\right) \left\{e^{-(y-0.25)^2 / 2\sigma^2} + e^{-(y-0.75)^2/2\sigma^2}\right\} \label{eq:kh_ic_a}\\
  v^\prime_B &= w_0\, \text{sin}\left(n_B\, \pi\, x\right). \label{eq:kh_ic_b}
\end{align}
Here $w_0 = 0.1$ is the scale of the velocity perturbation, $\sigma =
0.05/\sqrt{2}$ controls the width of the Gaussian for IC A, and
$\Delta y = 0.05$ is the transition distance scale for the smooth ramp of IC
B. The pressure everywhere is set to $p = 2.5$, and we run this with a
gamma-law equation of state set to $\gamma = 5/3$. Plotfiles are
generated every 0.05 seconds, and the problem is run until $t = 2$.

\begin{figure}[h!]
  \centering
  \includegraphics[trim=0.75in 0.45in 0.85in 0.5in, clip, scale = 0.75]{{{plots/kh_t2.0_p3_low_res_collage}}}
  \caption[Kelvin-Helmholtz instability and bulk velocity (McNally et al.)]
          {2D Kelvin-Helmholtz instability test at $t = 2.0$ for the initial 
           conditions given by \autoref{eq:kh_p3_rho} through \autoref{eq:kh_p3_vy},
           which come from McNally et al. (2012).
           The meaning of the rows and columns is the same as in \autoref{fig:kh_p2}.
           \label{fig:kh_p3}}
\end{figure}

We run the problem for $v_\text{bulk} = [0, 1, 3, 10, 30, 100]$, and
for each set of initial conditions run the problem at resolutions of
$64^2$, $128^2$, $256^2$, $512^2$. For context, in these units the 
sound speed is $c\approx 0.7$. In addition, for each initial
condition we run simulations at the higher resolutions of $1024^2$,
$2048^2$, and $4096^2$ for the stationary problem only. These serve 
as a reference solution to gauge the extent to which the bulk flow 
affects the development of the fluid instability, and to determine 
if the problem is numerically converged.

We find the same result as R10 for IC A, which is equivalent to the 
test proposed by S10: at low resolutions and high bulk velocity, the 
Kelvin-Helmholtz instability completely fails to develop. Furthermore 
the problem does not converge even qualitatively at the highest 
resolutions we used. Our results are very similar to Figure 3 of 
R10 so we do not show them here. For IC B, our results can be seen 
for the normal resolutions and all velocities in \autoref{fig:kh_p2}.
At low resolutions and very large bulk velocities, the fluid 
does get significantly disrupted by numerical error. This 
effect quickly converges away with resolution and qualitatively 
at $512^2$ resolution the solution is nearly identical to the stationary 
$v=0$ problem. We agree with R10 that this problem does converge 
with resolution and is not subject to numerically-seeded secondary 
instabilities at the stopping time. This is evident even at low resolutions
by examining the first row of \autoref{fig:kh_p2}.

\begin{figure}[h!]
  \centering
  \includegraphics[trim = 0in 1.25in 0.25in 1.25in, clip, scale=0.6]{{{plots/kh_p3_high_res_collage}}}
  \caption[High-resolution Kelvin-Helmholtz instability]
          {Time series of the Kelvin-Helmholtz problem proposed by McNally et al. (2012)
           as the simulation is just starting to go non-linear. The rows represent resolution, 
           where $n$ is the number of grid cells per spatial dimension, and the columns are 
           different snapshots in time.\label{fig:kh_p3_high_res}}
\end{figure}

\citet{mcnally:2012} published another Kelvin-Helmholtz problem that 
is well-posed in the sense that it converges with resolution and 
is not subject to uncontrollable numerical instabilities. Though they 
were not explicitly interested in the question of Galilean invariance, 
we visit that issue here to see what can be learned. The initial 
conditions for this problem are:
\begin{align}
  \rho &= \begin{dcases} \rho_1 - \rho_m e^{(y-0.25)/\Delta y} & 0.25 > y \geq 0 \\ 
                         \rho_2 + \rho_m e^{(0.25-y)/\Delta y} & 0.5 > y \geq 0.25 \\
                         \rho_2 + \rho_m e^{(y-0.75)/\Delta y} & 0.75 > y \geq 0.5 \\
                         \rho_1 - \rho_m e^{(0.75-y)/\Delta y} & 1 > y \geq 0.75 \end{dcases} \label{eq:kh_p3_rho}
\end{align}
\begin{align}
  v_x &= \begin{dcases} v_1 - v_m e^{(y-0.25)/\Delta y} & 0.25 > y \geq 0 \\
                        v_2 + v_m e^{(0.25-y)/\Delta y} & 0.5 > y \geq 0.25 \\
                        v_2 + v_m e^{(y-0.75)/\Delta y} & 0.75 > y \geq 0.5 \\
                        v_1 - v_m e^{(0.75-y)/\Delta y} & 1 > y \geq 0.75 \end{dcases} \label{eq:kh_p3_vx} \\
  v_y &= w_0\, \text{sin}\left(4\pi x\right). \label{eq:kh_p3_vy}
\end{align}
Here $\Delta y = 0.025$, $w_0 = 0.01$, $v_m = (v_1 - v_2) / 2$, $\rho_m = (\rho_1 - \rho_2) / 2$, 
and the other symbols have the same meaning as above (this means the flow direction
is reversed compared to the original paper, so as to achieve consistency with the 
other simulations presented here). We run this problem at all the same resolutions 
and bulk velocities as the previous two problems. The results for the normal resolutions 
at $t = 2.0$ are displayed in \autoref{fig:kh_p3}. We see a similar pattern as for 
the test proposed by R10: as we get to higher flow speeds we need to have higher 
spatial resolution to compensate for the increased numerical diffusion. The 
qualitative accuracy is much lower for the highest bulk velocities for this problem
than for the previous problems. This is because the amplitude of the instability overall
is smaller than for the previous problems, at least by $t = 2.0$, so it is easier
for numerical diffusion at the shearing layer, caused by the high bulk velocities,
to completely wipe out the instability. Like \citet{robertson:2010} found for
their problem, we find for this problem that the convergence properties are not
substantially affected by altering the perturbation frequency -- the results
show the same qualitative pattern even if we halve this frequency.

\citet{hopkins:2015} performed this test as part of the testing of
their code GIZMO.  They showed the late-time evolution of this system,
when non-linear effects have taken over and significantly disrupted
the initial flow. At low resolution the tested grid algorithm
had failed to disrupt both for $v = 0$ and $v = 10$. We too ran this
problem until $t = 10$, and confirm that the Kelvin-Helmholtz
instability damps out at low resolution but goes strongly non-linear
and disrupts the flow at high resolution.  We strongly
emphasize the point that this does not objectively demonstrate a
deficiency in fixed-grid codes for this problem. We can only determine
the validity of a method when we have a trustworthy, converged
solution to compare to, and this is lacking for this problem at late
times. As observed by \citeauthor{mcnally:2012}, this lack of a solution is because the
secondary instabilities form for this problem when the whorls of the
Kelvin-Helmholtz tendrils stretch out and create gradients that
approach the grid resolution. This is prime breeding ground for
numerical noise. But because the nature of this noise depends on
the resolution, it is very different for simulations at different
resolutions. If these instabilities are seeded because of this
resolution-dependent noise and are not seeded instead in a controlled
manner such that they appear at the same time and location, then we
simply cannot draw any conclusions that bear on the question of
verification from this test at late times.
\autoref{fig:kh_p3_high_res} provides a sense of this by examining the
crucial time at which the transition from the linear to the non-linear
regime is occurring.  At all of these very high resolutions the
secondary instabilities develop, but they occur at different times and
have different spatial scales for each resolution.

We conclude that large bulk motions of fluid can have very significant effects 
on numerical calculations of shear mixing in fixed-grid codes, but that this effect 
diminishes with increasing resolution. As a result, we must be confident that we are 
sufficiently resolving the major mixing regions on the white dwarf surfaces,
specifically that the density gradients occur over spatial scales much larger
than the grid resolution. If we find instead that this mixing occurs near the
grid resolution scale, this will imply that we need to ramp up the resolution
in these regions using AMR. If this becomes too expensive, we would need to be
skeptical of any conclusions that could be drawn 
about the effect of the mixing on the nuclear burning.

\subsubsection{Moving Star}
\label{sec:moving_star}

To analyze the effects of velocity-dependent results for a stellar simulation, we repeated the
test of \autoref{sec:HSE} with a bulk velocity on the grid. We chose a
velocity of $2.56 \times 10^{8}\ \text{cm s}^{-1}$. For context, this is 
comparable to the orbital velocities of the stars in \autoref{sec:kepler}, and the Mach
number is of order unity in the stellar core at this speed.  This test
was inspired by \citet{tasker:2008}, who considered a moving galaxy
cluster and who obtained a long timescale evolution by using
periodic boundary conditions, so that the cluster would cross the
domain multiple times throughout the evolution.  We believe that
periodic boundary conditions are unrealistic for our type of
simulation, so we prefer to do one continuous simulation where the
star does not cross the boundaries.  Since our normal grid was not
large enough to allow the motion to continue for very long, we
expanded the domain size by a factor of four, and then included an
extra refined level around the star to keep the effective resolution
the same. We started the star off in the lower left corner of the
domain, and pointed its velocity towards the upper right
corner. This allowed us to evolve the star for the same length of
time as for the original test. We note that getting the gravity
boundary conditions right required us to move the origin of the
problem at the bulk velocity, so that the multipole moments were
always computed with respect to the current location of the stellar center.

\begin{figure}[h!]
  \centering
  \includegraphics[scale=0.8,trim=0.1in 0.0in 0.6in 0.6in,clip]{plots/single_star_compare_1e3_radius}
  \caption[Moving star versus static star]
          {A variation on \autoref{fig:single_star_static_radius} where
           we now compare the ``static'' case to ``motion'' simulations where the 
           star moves across the grid at a fixed linear speed. The lines represent 
           the effective number of zones per dimension inside the stellar material;
           due to the expanded size of the grid in the ``motion'' case, the 
           physical resolution is the same in each column in the legend.
           \label{fig:single_star_compare_radius}}
\end{figure}

In \autoref{fig:single_star_compare_radius}, we take the results of
\autoref{sec:HSE} (the ``static'' case), and plot on top of it the
results of this new simulation (the ``motion'' case). We see
immediately that this bulk velocity causes the star to be much worse
at maintaining hydrostatic equilibrium. Not only is the absolute size
of the star significantly larger (nearly a factor of two at the lowest
feasible resolution we consider), but also there is a clear upward
trend in the size that has not terminated at any resolution by the end
of the simulation.  This again emphasizes the results mentioned
earlier, that we must be careful not to trust any simulation with
significant mass transfer if we are not confident that the mass
transfer is seeded in a controllable manner and free from numerical
noise.

\subsection{Keplerian Orbit}
\label{sec:kepler}

We now consider the phase of the binary system where the stars are orbiting each other 
at distances great enough that the initial orbits should be approximately Keplerian. 
There are a number of effects worth looking into here. For simplicity, we choose two 
cases to demonstrate the simulation behavior: an equal mass case of two $0.9\ \msolar$ 
white dwarfs, and an unequal mass case of $0.9\ \msolar$ and $0.75\ \msolar$ white dwarfs.
In both systems, the secondary should be stable against mass loss.
In each case, the initial orbital period is 100 seconds.

For some of the algorithms described earlier in this work, a single orbit of these 
systems is enough to examine their effects. In \autoref{sec:gravity_boundary_conditions},
we discussed the replacement of a monopole boundary condition solver for the gravitational 
potential with a more general multipole solver for the boundaries. To test the relevance 
of this effect, we considered a single orbit of the unequal mass system and measured 
the distance between the two white dwarf centers of mass at the beginning of the simulation and after 
the full orbital period. This distance should not change significantly over that timescale.
We performed this test for maximum multipole moments ranging from 0 (the monopole term) to 16.
The results are shown in \autoref{fig:gravity_bcs}. Terms in the boundary potential 
that vary faster than $r^{-5}$ are effectively negligible in determining the outcome of the orbit, 
justifying our typical choice of maintaining terms up to $r^{-7}$.

\begin{figure}[h!]
  \centering
  \includegraphics[scale=0.8,trim=0.1in 0.0in 0.65in 0.4in,clip]{plots/gravity_bcs}
  \caption[Distance change due to multipole boundary conditions]
          {Absolute magnitude of the relative change in the distance of two unequal mass white dwarfs after one orbital period. 
           The stars were evolved in an inertial reference frame. The horizontal axis is the number of terms or multipole moments 
           captured in the series expansion for the potential at the domain boundary.\label{fig:gravity_bcs}}
\end{figure}

Another diagnostic that we consider is the energy conservation of the
system. Recalling \autoref{sec:gravity_hydro_coupling}, there are
several different methods of applying the gravitational source term to
the hydrodynamics equations. In \castro\ we presently have four
options, controlled by the parameter {\tt castro.grav\_source\_type},
which we shorten to {\tt gs} for the present discussion. 
${\tt gs} = 1$ and ${\tt gs} = 2$ are variations on the standard 
cell-centered source term for gravity. The difference between them is that 
${\tt gs} = 2$ determines the value of the energy source term after the momentum
source term has been applied, while ${\tt gs} = 1$ uses the uncorrected
momenta in calculating $\rho \mathbf{u} g$. We have found ${\tt gs} = 2$ to be
more accurate. ${\tt gs} = 3$ is entirely different: after calculating
the new momenta, we reset the total energy to be equal to the internal
energy plus the kinetic energy. This approach has the virtue of ensuring that
there is no conflict due to discretization between the momentum and
energy equations, and also correctly ensuring that the gravitational
force does not directly change the internal energy---and thus the
temperature---of the fluid. However, it explicitly sacrifices total
energy conservation. ${\tt gs} = 4$ is the new conservative method of
evaluating the energy source terms at cell faces. The results for the
change in energy after a single orbit are seen in the first column of
\autoref{table:sources}. The first two versions give reasonable and
similar levels of energy conservation. The third has total energy
changes on the order of 100\%, but this itself does not have a severe
effect on the dynamics because in this scheme the total energy
variable is effectively a placeholder value of the kinetic energy plus
internal energy, rather than being evolved directly. The last scheme
is nearly two orders of magnitude better in energy conservation,
justifying the effort in varying the scheme.

In \autoref{table:sources} we show also the effects on energy conservation of using the inertial reference frame. 
We use ${\tt rs}$ for the \castro\ parameter {\tt castro.rot\_source\_type}.
Each option for ${\tt rs}$ is implemented in the same way as for
the gravitational source term, simply swapping out the gravitational acceleration
for the rotational acceleration (except for the improvement to the momentum update
for ${\tt rs} = 4$ described in \autoref{sec:rotation}). 
The ${\tt rs} = 0$ column means that rotation is turned off and we are 
in the inertial frame. We see that the choice of rotational coupling is much less important than the choice of gravity coupling. 
The ``conservative'' ${\tt rs} = 4$ is slightly better in energy conservation than the non-conservative, 
cell-centered ${\tt rs} = 2$ algorithm, but it is a small effect.

\input{plots/sources.table}

We are most interested in the stability of these systems over long
timescales. To this end, we consider the same systems as above, but
evolve them for 25 orbital periods. In
\autoref{fig:circular_orbit_comparison} we illustrate the evolution of
these systems by plotting the center of mass locations of the white
dwarfs on the orbital ($xy$) plane. For the equal mass case in the
inertial reference frame, the curves fall nearly on top of each
other for most of the run, indicating that the stars are indeed 
orbiting at the initial distance, at least for a while. Towards the end 
of the run, however, the orbit starts to decay significantly, and the center-of-mass
distance of the two stars has decreased by about 10\% after 25 orbits.
We attribute this to non-conservation of angular momentum, which occurs 
because our code only explicitly conserves linear momentum. This orbital 
decay resembles the effect seen by \citet{swc:2000} for the case of neutron stars. 
In the unequal mass case, the magnitude of the orbital decay is smaller but 
at the end of the run the secular decline in distance is also visible. In 
both cases the stars would likely merge due to numerical error 
after a long enough timescale.

The co-rotating frame is different.  For clarity of visualization, we
rotate these results back into the inertial frame before displaying
their orbits.  In both the equal and unequal mass cases, the centrifugal force 
pushes the stars outward toward a new equilibrium distance that is a few 
percent larger than its initial distance. At the end of the run, the system is 
relatively stable, with oscillations about the new equilibrium distance. In fact 
these oscillations occur too in the inertial frame, but they are much more pronounced 
here. In the unequal mass case this is coupled with severe precession of the orbit, 
which results in chatoic-looking orbits when viewed from the rotating reference frame. 
These result from the explicit numerical consideration of the Coriolis and centrifugal 
terms, which do not appear in the inertial frame. So while the rotating frame 
is clearly more stable against mass transfer than the inertial frame,
the cost is that the specific dynamics may be more suspect.

\begin{figure}[h!]
  \centering
  \includegraphics[scale=0.8,trim=0.1in 0.0in 0.1in 0.0in,clip]{plots/circular_orbit_comparison}
  \caption[White dwarf positions over 25 orbital periods]
          {Positions of the white dwarfs in the orbital plane for four
           cases evolved over 25 orbital periods.  The $x$ and $y$ axes are
           normalized to the size of the domain, so that $x = -0.5$ is the
           left edge and $x = 0.5$ is the right edge. The dashed blue curve
           is the position of the primary white dwarf, and the solid red
           curve is the position of the secondary. In plot (a) we have the
           equal mass system evolved in the inertial reference frame, and in
           plot (c) we have the same system evolved in a rotating frame,
           where the positions have been transformed back to the inertial
           frame for comparison.  Plots (b) and (d) are analogous but for the
           unequal mass system.\label{fig:circular_orbit_comparison}}
\end{figure}

Turning to the conservation properties of the system, we examine as
fairly typical cases the equal mass system in the inertial frame for
energy conservation (\autoref{fig:energy_conservation_equal}), and the
unequal mass system in the rotating frame for angular momentum
conservation (\autoref{fig:angular_momentum_conservation_unequal}).
For the former system angular momentum is conserved to within 10 percent over the 25
orbits, while energy conservation is about an order of magnitude
better. We note that while this is already a fairly good level of
energy conservation, it is not nearly as good as the results of
\citet{marcello:2012}. This is because we reset the internal energy to
a level corresponding to our temperature floor when it goes negative,
while \citeauthor{marcello:2012} do not reset and instead ignore the
internal energy if it is negative. The resets impose an artificial
floor on our ability to conserve energy, but they only happen in
low-density regions and do not much affect the large-scale dynamics.
Meanwhile, relative angular momentum conservation is not quite as good 
as relative energy conservation.  This is 
linked to the decline (or increase) in the size of the orbit. This
implies that we ought to be careful in concluding that at these
moderate resolutions we can safely evolve systems for many dozens of
orbits; this needs to be verified to ensure that an observed inspiral
and merger is physically (not numerically) motivated.

\begin{figure}[h!]
  \centering
  \includegraphics[scale=0.8,trim=0.1in 0.0in 0.1in 0.1in,clip]{plots/equal_energy_rot0}
  \caption[System energy over 25 orbital periods]
          {Absolute magnitude of the relative change in energy of two equal mass white dwarfs through 25 orbital periods,
           evolved in an inertial reference frame. The decline and recovery is a change in sign of the energy difference.
           \label{fig:energy_conservation_equal}}
\end{figure}

\begin{figure}[h!]
  \centering
  \includegraphics[scale=0.8,trim=0.1in 0.0in 0.1in 0.1in,clip]{plots/unequal_angular_momentum_rot1}
  \caption[System angular momentum over 25 orbital periods]
          {Absolute magnitude of the relative change in angular momentum of two unequal mass white dwarfs after 25 orbital periods, 
           evolved in a co-rotating reference frame. We consider only the component of the angular moment along the rotational 
           axis.
           \label{fig:angular_momentum_conservation_unequal}}
\end{figure}

As a simple verification test to ensure our gravitational wave calculations are correct, we plot the 
gravitational wave strain along the rotation axis for the first two periods of an unequal mass system. 
At this early time the orbit is circular and so to a good approximation we expect that the gravitational 
wave signal should be that of two point masses, whose positions are:
\begin{align}
  \mathbf{r}_P(t) &= -a_P\, \text{cos}(\omega t) \hat{x} - a_P\, \text{sin}(\omega t) \hat{y} \\
  \mathbf{r}_S(t) &= a_S\, \text{cos}(\omega t) \hat{x} + a_S\, \text{sin}(\omega t) \hat{y}.
\end{align}
Then the mass distribution is $\rho(\mathbf{r}) = M_P\, \delta^3(\mathbf{r} - \mathbf{r}_P) + M_S\, \delta^3(\mathbf{r} - \mathbf{r}_S)$.
From this it is straightforward to calculate the quadruopole tensor, take its second time derivative, and then apply the 
projection operator to get the gravitational wave polarizations along the rotation axis:
\begin{align}
  h_+ &= -4\frac{G\mu}{c^4 r}\left[G M_{\text{tot}} \omega \right]^{2/3}\, \text{cos}(2\omega t) \\
  h_\times &= -4\frac{G\mu}{c^4 r}\left[G M_{\text{tot}} \omega \right]^{2/3}\, \text{sin}(2\omega t).
\end{align}
$\mu$ is the reduced mass, while $M_{\text{tot}}$ is the total mass. From this we see that the 
gravitational wave frequency is twice the orbital frequency, and that the two polarizations 
are out of phase by $90^\circ$ in time. We compare this analytical expectation to the 
numerical results in \autoref{fig:gw_strain}. We find very good agreement in this case, and this 
level of agreement holds in the rotating frame as well.

\begin{figure}[h!]
  \centering
  \includegraphics[scale=0.8,trim=0.1in 0.0in 0.1in 0.2in,clip]{plots/unequal_gw_rot0}
  \caption[Gravitational wave strain test]
          {Gravitational wave strain polarizations for the first two orbital periods of an 
           unequal mass system. The curves with markers are the numerical data, while the 
           curves without markers are the analytical results for two point masses.\label{fig:gw_strain}}
\end{figure}

Finally we consider whether the dynamical behavior of the system converges with resolution. 
In \autoref{fig:unequal_spatial_convergence_inertial} we plot the first full orbit for 
the unequal mass system, at three different resolutions in the inertial frame: our default 
resolution of $256^3$ zones, as well as a single level of refinement with a jump by a factor of 
two (effective resolution $512^3$) or a jump by a factor of four (effective resolution $1024^3$). 
It is clear that at the latter resolution (corresponding to physical resolution of 100 km), 
we have achieved convergent behavior. In the rotating frame, the results also show
convergent behavior but the convergence is not as fast with resolution as in the inertial frame; 
see \autoref{fig:unequal_spatial_convergence_rotating}. At the two higher resolutions the white dwarf
distance is qualitatively similar, and both are qualitatively different from the lower resolution. However,
quantitatively the two higher resolution runs are not as similar to each other as the analogous runs in the
inertial frame. Convergence with resolution is slightly slower in the rotating reference frame
because in the rotating reference frame a stable, unchanging circular orbit requires balance between
two forces with opposite sign (the gravitational and centrifugal forces), and slight perturbations from the
circular orbit are amplified by the effect of the Corolis force. In the inertial frame, these numerical instabilities
vanish, but the cost is that there is no centrifugal force to actively maintain the white dwarf distance,
which is why it is much more likely for the orbit to prematurely decay. In either case, these results suggest
at least a minimum resolution of 200 km for getting the dynamics qualitatively right. To put that into context,
consider that the parameter study of \citet{dan:2014} used 40,000 SPH particles per simulation, or (for an equal mass
binary) 20,000 particles per white dwarf. For, say, a $0.9\ \msolar + 0.9\ \msolar$ white dwarf binary on
a $256^3$ zone simulation grid, there are 20,000 zones that fit within a white dwarf. We do not
intend here to directly compare results between the two simulation methods. We limit ourselves to the
observation that at least for grid-based codes, a parameter study such as the ones performed by
\citet{dan:2012} and \citet{dan:2014} would likely not yield qualitatively convergent results if it were to use the
same effective mass resolution. Instead the number of zones inside each star should at least be doubled.

\begin{figure}[h!]
  \centering
  \includegraphics[scale=0.8,trim=0.1in 0.0in 0.1in 0.2in,clip]{plots/spatial_convergence_rot0}
  \caption[Distance between two unequal mass white dwarfs, inertial frame]
          {Distance between the two white dwarfs in the unequal mass system, for the first orbit.
           The distance is scaled by the initial orbital distance. 
           We plot at three different resolutions, corresponding to the number of 
           effective zones per dimension in the refined regions.
           \label{fig:unequal_spatial_convergence_inertial}}
\end{figure}

\begin{figure}[h!]
  \centering
  \includegraphics[scale=0.8,trim=0.1in 0.0in 0.1in 0.2in,clip]{plots/spatial_convergence_rot1}
  \caption[Distance between two unequal mass white dwarfs, rotating frame]
          {Distance between the two white dwarfs in the unequal mass system, for the first orbit.
           The distance is scaled by the initial orbital distance. 
           We plot at three different resolutions, corresponding to the number of 
           effective zones per dimension in the refined regions.
           \label{fig:unequal_spatial_convergence_rotating}}
\end{figure}



\newpage
\section{Parallel Performance}
\label{sec:performance}



\newpage
\section{White Dwarf Collisions}
\label{sec:collisions}



\subsection{Parameter Study}
\label{sec:collisions_parameters}

\subsubsection{Timestepping}
\label{sec:collision_parameters:timestepping}

\subsubsection{Nuclear Network}
\label{sec:collision_parameters:network}

\subsubsection{Burning Mode}
\label{sec:collision_parameters:burningmode}

\subsubsection{Impact Parameter}
\label{sec:collision_parameters:impactparameter}

\subsubsection{Other Parameters}
\label{sec:collision_parameters:other}



\subsection{Resolution Dependence}
\label{sec:collisions_resolution}



\subsection{Gravitational Wave Signature}
\label{sec:collisions_gravitational_waves}



\newpage
\section{White Dwarf Mergers}
\label{sec:mergers}



\newpage
\section{Discussion and Conclusions}
\label{sec:conclusion}



\newpage
\bibliographystyle{aasjournal}
\bibliography{refs}


\newpage

\appendix



\section{Proof of Energy Conservation in Simulations using Self-Gravity}
\label{app:gravity}

In \autoref{sec:gravity_hydro_coupling}, we described our approach to updating the gas energy
in response to motions of fluid through the self-generated gravitational potential using 
\autoref{eq:grav_energy_conservation_update}. While it is straightforward to observe that this approach
should be conservative for an arbitrary fixed external potential $\Phi$, it is not as obvious that this
should be so for a self-generated potential which changes in response to mass motions on the domain. To
see that this still holds for the self-generated gravitational potential $\Phi$, let us start with 
\autoref{eq:grav_energy_conservation_update} in a slightly revised form:
\begin{equation}
  \Delta(\rho E)_i = -\frac{1}{2}\sum_{j} \Delta\rho_{ij}(\Phi_i - \Phi_{j}) \label{eq:grav_energy_conservation_update_revised}
\end{equation}
where by $\Delta \rho_{ij}$ we mean the density transferred from zone $j$ to zone $i$, so that
$\Delta \rho_{ij} = - \Delta \rho_{ji}$, and the sum is over all zone indices $j$ that are adjacent
to zone $i$. Let us define $\Phi_{ij} = \Phi_{ji} = (\Phi_{i} + \Phi_{j}) / 2$ as the potential on the
zone interface between zones $i$ and $j$. Then we have:
\begin{equation}
  \Delta(\rho E)_i = -\sum_{j} \Delta\rho_{ij}(\Phi_i - \Phi_{ij}).
\end{equation}
We can evalute the sum for all of the terms proportional to $\Phi_i$ by observing that the change in
density from time-level $n$ to time-level $n+1$ is the sum of the density fluxes from all adjacent zones.
\begin{equation*}
  \Delta(\rho E)_i = - (\rho_i^{n+1} - \rho_i^{n}) \Phi_i + \sum_{j}\Delta \rho_{ij} \ \Phi_{ij}
\end{equation*}
Now let us sum this over all zones $i$ in the domain, and ignore the domain boundaries, or assume that they are
far enough away from the region of compact support for $\rho$ that $\Phi$ is negligible there. As the second
term on the right-hand side is antisymmetric in $i$ and $j$, it cancels when summing adjacent zones, and we have:
\begin{equation*}
  \sum_{i} (\rho E)_i^{n+1} - \sum_{i} (\rho E)_i^{n} = -\frac{1}{2}\sum_{i} (\Phi_{i}^{n+1} + \Phi_{i}^{n})(\rho_i^{n+1} - \rho_i^{n})
\end{equation*}
Note that, as explained the text, we are using a time-centered $\Phi$ to correspond to the mass fluxes
at time-level $n+1/2$. Finally we re-write this in a form where the difference in total energy between time-levels
$n$ and $n+1$ is on the left-hand side and any sources causing this to be non-zero are on the right-hand side:
\begin{align}
  \sum_{i} \left(\rho E + \frac{1}{2}\rho\Phi\right)_i^{n+1} - \sum_{i} \left(\rho E + \frac{1}{2}\rho\Phi\right)_i^{n} &= \frac{1}{2}\sum_{i} \left(\Phi_i^{n+1}\rho_i^{n} - \Phi_i^{n}\rho_i^{n+1}\right) \notag \\
       &= \frac{1}{8\pi G} \sum_{i}\left(\Phi_{i}^{n+1}\nabla^2 \Phi_{i}^{n} - \Phi_i^{n}\nabla^2 \Phi_{i}^{n+1}\right) \label{eq:total_energy_difference}
\end{align}
\autoref{eq:total_energy_difference} expresses total energy conversation if and only if the right-hand side vanishes.
We observe that the right-hand side has the form of a variant of the divergence theorem often called Green's second identity:
\begin{equation}
  \int (\Phi^{n}\nabla^2 \Phi^{n+1} - \Phi^{n+1}\nabla^2 \Phi^{n}) dV = \int \left(\Phi^{n} \nabla \Phi^{n+1} - \Phi^{n+1} \nabla \Phi^{n}\right) \cdot d\mathbf{S}, \label{eq:green_second_identity}
\end{equation}
where $d\mathbf{S}$ is the area element with vector component parallel to the outward normal. The analogous result holds for the
discretized form in \autoref{eq:total_energy_difference}. With the assumptions used above, the right-hand side of
\autoref{eq:green_second_identity} will vanish as the surface integral is evaluated at infinity, where the potential
tends to zero. This concludes the proof that the method is conservative when the potential used at the zone interfaces
is time-centered, even in light of the change of the potential over the timestep due to the mass motion that is causing the change in the energy.

From the above discussion it is straightforward to see exactly why the method is not fully conservative to machine
precision in practice. First, we cannot simulate the domain out to infinity, so Green's second identity does not hold exactly
and there is some loss or addition of energy at domain boundaries. Second, \autoref{eq:total_energy_difference} holds in the
continuum limit by using the Poisson equation, but in practice it is not exactly true that $\rho_i = 4\pi G \nabla^2 \Phi_{i}$ due
to small errors in the potential at the level of the tolerances used in the Poisson solver.



\newpage
\section{Formulation of the Multipole Expansion for the Gravitational Potential}
\label{app:multipole}

The integral formulation of the gravitational potential, using a series expansion in spherical harmonics, is:
\begin{equation}
  \Phi(\mathbf{x}) = -G\sum_{l=0}^{\infty} \sum_{m=-l}^{l} \frac{4\pi}{2l+1} \int \rho(\mathbf{x}^\prime)\, Y_{lm}(\theta,\phi)\, Y_{lm}^*(\theta^\prime,\phi^\prime)\, \frac{r_{<}^{l}}{r_{>}^{l+1}}\, dV^\prime,
\end{equation}
where $\theta$ is the polar angle and $\phi$ is the azimuthal angle, $r \equiv |\mathbf{x}|$ is the radial distance, and at any point in the domain $r_{<}$ is the smaller of $r$ and $r^\prime$, and $r_{>}$ is the larger of the two. This immediately suggests writing the potential at any location as the sum of
two series:
\begin{equation*}
  \Phi(\mathbf{x}) = -G\sum_{l=0}^{\infty} \sum_{m=-l}^{l} \frac{4\pi}{2l+1}\left[ q^{L}_{lm}(\mathbf{x})\, r^{-l-1} + q^{U}_{lm}(\mathbf{x})\, r^{-l-1} \right] Y_{lm}(\theta,\phi),
\end{equation*}
where we have defined two multipole moments as integrals over the domain:
\begin{align}
  q^{L}_{lm}(\mathbf{x}) &= \int dV^\prime\, \rho(\mathbf{x}^\prime)\, Y^*(\theta^\prime,\phi^\prime)\, \Theta(r - r^\prime)\, {r^\prime}^{l} \\
  q^{U}_{lm}(\mathbf{x}) &= \int dV^\prime\, \rho(\mathbf{x}^\prime)\, Y^*(\theta^\prime,\phi^\prime)\, \Theta(r^\prime - r)\, {r^\prime}^{-l-1}.
\end{align}
$\Theta(r)$ is the standard step function, equal to one if the argument is positive and zero if the argument is negative. Geometrically,
$q^{L}(\mathbf{x})$ is an integral containing only mass interior to $|\mathbf{x}|$, and $q^{U}(\mathbf{x})$ is an integral containing only
mass exterior to $|\mathbf{x}|$. Provided that one has computed these two integrals for a point $\mathbf{x}$, one can use the series expansion
to calculate the potential at that point in principle to arbitrary accuracy by including higher order terms.
  
We prefer to work with solely real-valued quantities, and so we make use of the addition theorem for spherical harmonics \citep[Section 3.6]{jackson}:
\begin{align}
  \frac{4\pi}{2l+1} \sum_{m=-l}^{l} Y^*_{lm}(\theta^\prime,\phi^\prime)\, Y_{lm}(\theta, \phi) &= P_l(\text{cos}\, \theta) P_l(\text{cos}\, \theta^\prime) \notag \\
   &\hspace{-2.0in}+ 2 \sum_{m=1}^{l} \frac{(l-m)!}{(l+m)!} P_{l}^{m}(\text{cos}\, \theta)\, P_{l}^{m}(\text{cos}\, \theta^\prime)\, \left[\text{cos}(m\phi)\, \text{cos}(m\phi^\prime) + \text{sin}(m\phi)\, \text{sin}(m\phi^\prime)\right].
 \end{align}
The $P_l(x)$ are the Legendre polynomials and the $P_l^m(x)$ are the associated Legendre polynomials. We construct them using a stable recurrence relation given known values for $l = 0$ and $l = 1$. We can then formulate the expansion in a different way:
\begin{align}
  \Phi(\mathbf{x}) &= -G\sum_{l=0}^{\infty} \left\{ Q_{l}^{(L,0)}(\mathbf{x})\, P_l(\text{cos}\, \theta)\, r^{-l-1} + Q_{l}^{(U,0)}(\mathbf{x})\, P_l(\text{cos}\, \theta)\, r^{l} \right. \notag \\
  &\hspace{0.7in} + \sum_{m=1}^{l} \left[ Q_{lm}^{(L,C)}(\mathbf{x})\, \text{cos}(m\phi) + Q_{lm}^{(L,S)}(\mathbf{x})\, \text{sin}(m\phi)\right] P_{l}^{m}(\text{cos}\, \theta)\, r^{-l-1} \notag \\
  &\hspace{0.7in} + \left. \sum_{m=1}^{l} \left[ Q_{lm}^{(U,C)}(\mathbf{x})\, \text{cos}(m\phi) + Q_{lm}^{(U,S)}(\mathbf{x})\, \text{sin}(m\phi)\right] P_{l}^{m}(\text{cos}\, \theta)\, r^{l} \right\}
\end{align}

The multipole moments now take the form:
\begin{align}
  Q_l^{(L,0)}(\mathbf{x}) &= \int P_l(\text{cos}\, \theta^\prime)\, \Theta(r - r^\prime)\, {r^{\prime}}^l \rho(\mathbf{x}^\prime)\, d^3 x^\prime \\
  Q_l^{(U,0)}(\mathbf{x}) &= \int P_l(\text{cos}\, \theta^\prime)\, \Theta(r^\prime - r)\, {r^{\prime}}^l \rho(\mathbf{x}^\prime)\, d^3 x^\prime \\  
  Q_{lm}^{(L,C)} &= 2\frac{(l-m)!}{(l+m)!} \int P_{l}^{m}(\text{cos}\, \theta^\prime)\, \text{cos}(m\phi^\prime)\, \Theta(r - r^\prime)\, {r^\prime}^l \rho(\mathbf{x}^\prime)\, d^3 x^\prime \\
  Q_{lm}^{(U,C)} &= 2\frac{(l-m)!}{(l+m)!} \int P_{l}^{m}(\text{cos}\, \theta^\prime)\, \text{cos}(m\phi^\prime)\, \Theta(r^\prime - r)\, {r^\prime}^{-l-1} \rho(\mathbf{x}^\prime)\, d^3 x^\prime \\
  Q_{lm}^{(L,S)} &= 2\frac{(l-m)!}{(l+m)!} \int P_{l}^{m}(\text{cos}\, \theta^\prime)\, \text{sin}(m\phi^\prime)\, \Theta(r - r^\prime)\, {r^\prime}^l \rho(\mathbf{x}^\prime)\, d^3 x^\prime \\
  Q_{lm}^{(U,S)} &= 2\frac{(l-m)!}{(l+m)!} \int P_{l}^{m}(\text{cos}\, \theta^\prime)\, \text{sin}(m\phi^\prime)\, \Theta(r^\prime - r)\, {r^\prime}^{-l-1} \rho(\mathbf{x}^\prime)\, d^3 x^\prime.  
\end{align}
In practice, of course, we select some maximum value $l_{\text{max}}$ at which we terminate the summation, determined either by computational efficiency requirements or by the fact that there is little information at high orders for sufficiently smooth mass distributions. In \castro\ we have the capability to compute any of the above multipole moments, though in this paper we are only using the multipole expansion to calculate the boundary conditions on the potential, and so we neglect calculation of the moments with a $U$ subscript as we are assuming that all of the mass is interior to the boundary. \autoref{eq:multipole_potential} is directly recovered under these conditions.

\clearpage

\end{document}
